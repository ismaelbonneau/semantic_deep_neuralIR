{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118194 concepts, 118422 relations\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "concepts = pickle.load(open(\"data/concepts.pkl\", 'rb'))\n",
    "relations = pickle.load(open(\"data/relations.pkl\", \"rb\"))\n",
    "\n",
    "print(\"{} concepts, {} relations\".format(len(concepts), len(relations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "G.add_nodes_from(concepts)\n",
    "G.add_edges_from(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thing', 'Organisation', 'Company', 'Pixar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.shortest_path(G, \"Thing\", \"Pixar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "conceptsEmbeddings = Doc2Vec.load(\"embeddings/concepts\")\n",
    "conceptsEmbeddings.init_sims(True)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wordEmbeddings = Word2Vec.load('embeddings/model_1')\n",
    "wordEmbeddings.init_sims(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {}\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string,remove_stopwords,strip_numeric, strip_tags, strip_punctuation, strip_short, strip_multiple_whitespaces\n",
    "from krovetzstemmer import Stemmer\n",
    "ks = Stemmer()\n",
    "\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags,\n",
    "                  strip_multiple_whitespaces, strip_punctuation, remove_stopwords, lambda x: ks.stem(x)]\n",
    "\n",
    "with open(\"data/topics-title.annotated_rectified.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        q_id = line.split(\"\\t\")[0]\n",
    "        query = line.split(\"\\t\")[1].strip()\n",
    "        if \"$#!\" in query:\n",
    "            tokens = query.split(\" \")\n",
    "            queries[q_id] = []\n",
    "            for i in range(len(tokens) - 1):\n",
    "                w = tokens[i]\n",
    "                if \"$#!\" in tokens[i+1]:\n",
    "                    tiascompris = preprocess_string(w, CUSTOM_FILTERS)\n",
    "                    if tiascompris != []:\n",
    "                        queries[q_id].append((tiascompris[0], tokens[i+1]))\n",
    "                elif \"$#!\" not in w and \"$#!\" not in tokens[i+1]:\n",
    "                    tiascompris = preprocess_string(w, CUSTOM_FILTERS)\n",
    "                    if tiascompris != []:\n",
    "                        queries[q_id].append((tiascompris[0], \"\"))\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hubble', ''),\n",
       " ('telescope', '$#!Telescope'),\n",
       " ('achievement', '$#!Accomplishment')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries['303']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs chargés\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "docs = {}\n",
    "collections = [\"FR94\", \"FT\", \"FBIS\", \"LATIMES\"]\n",
    "for collection in collections:\n",
    "    with open(\"data/annotatedrobust2004\"+collection+\".json\", \"r\") as f:\n",
    "        docs.update(json.load(f))\n",
    "print(\"docs chargés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance chargé\n"
     ]
    }
   ],
   "source": [
    "paires = {}\n",
    "with open(\"data/qrels.robust2004.txt\",\"r\") as f:\n",
    "    for line in f :\n",
    "        l = line.strip().split(' ')\n",
    "        paires.setdefault(l[0],{})\n",
    "        paires[l[0]].setdefault('relevant',[])\n",
    "        paires[l[0]].setdefault('irrelevant',[])\n",
    "        if l[-1]=='1':\n",
    "            paires[l[0]]['relevant'].append(l[2])\n",
    "        else:\n",
    "            paires[l[0]]['irrelevant'].append(l[2])\n",
    "print(\"relevance chargé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def hist(query, document):\n",
    "    cos = np.dot(query, document.T)\n",
    "    return np.apply_along_axis(lambda x: np.log10(1 + np.histogram(x, bins=30, range=(-1,1))[0]), 1, cos) #log de l'histogramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = [q for q in self.d_query.keys() if q in self.paires]\n",
    "query_idf = {}\n",
    "for id_requete in lol:\n",
    "    query_idf[id_requete] = self.get_idf_vec(custom_tokenizer(self.d_query[id_requete]))\n",
    "pickle.dump(query_idf, open(\"saved_data/query_idf.pkl\", \"wb\"))\n",
    "del query_idf\n",
    "\n",
    "print(\"nombre de requetes: %d.\" % len(lol)) \n",
    "tiascompris = list(self.docs.keys())\n",
    "lol.remove(\"634\")\n",
    "\n",
    "for id_requete in lol:\n",
    "#recuperer les mots dont on connait les embeddings dans la query\n",
    "    query_embeddings = np.zeros((self.max_length_query, 300))\n",
    "    i = 0\n",
    "    for word in self.d_query[id_requete].split():\n",
    "        if word in self.model_wv:\n",
    "            query_embeddings[i] = self.model_wv[word]\n",
    "        i += 1\n",
    "    query_embeddings = np.array(query_embeddings)\n",
    "\n",
    "    interractions = []\n",
    "\n",
    "    for pos in self.paires[id_requete][\"relevant\"]:\n",
    "    #lire le doc, la requete et creer l'histogramme d'interraction\n",
    "        pos_embeddings = []\n",
    "        for word in self.docs[pos]['text'].split():\n",
    "            if word in self.model_wv:\n",
    "                pos_embeddings.append(self.model_wv[word])\n",
    "        pos_embeddings = np.array(pos_embeddings)\n",
    "\n",
    "        interractions.append(self.hist(query_embeddings, pos_embeddings)) #append le doc positif\n",
    "\n",
    "        neg = np.random.choice(self.paires[id_requete][\"irrelevant\"], 1, replace=False)[0]\n",
    "        neg_embeddings = []\n",
    "        for word in self.docs[neg]['text'].split():\n",
    "            if word in self.model_wv:\n",
    "                neg_embeddings.append(self.model_wv[word])\n",
    "            neg_embeddings = np.array(neg_embeddings)\n",
    "\n",
    "        #interractions.append(self.hist(query_embeddings, pos_embeddings)) #append le doc négatif\n",
    "        interractions.append(self.hist(query_embeddings, neg_embeddings)) #append le doc négatif\n",
    "\n",
    "        print(\"requete %s complete.\" % id_requete)\n",
    "\n",
    "\n",
    "    np.save(\"saved_data2/\"+id_requete+\"_interractions.npy\", np.array(interractions))\n",
    "print(\"data completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paires['301']['relevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('international', '$#!International'),\n",
       " ('organized', ''),\n",
       " ('crime', '$#!Crime')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[\"301\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t $#!Language\n",
      "$#!British_Forces_Broadcasting_Service\n",
      "$#!Government\n",
      "$#!Evidence_(law)\n",
      "$#!Mafia\n",
      "$#!Caporegime\n",
      "$#!Justice\n",
      "$#!Trade\n",
      "$#!Mafia\n",
      "\t\t\t\t\t $#!Colombia\n",
      "$#!Justice\n",
      "$#!Government\n",
      "\t\t\t\t\t $#!The_Doors\n",
      "\t\t\t\t\t $#!Gustavo_de_Greiff\n",
      "$#!In_Touch_(radio_series)\n",
      "$#!News_media\n",
      "$#!Junius_P._Rodriguez\n",
      "$#!President_of_France\n",
      "$#!Terrorism\n",
      "$#!Authority\n",
      "$#!Process_philosophy\n",
      "$#!Evidence\n",
      "$#!Mafia\n",
      "$#!Indictment\n",
      "$#!Warrant_(law)\n",
      "$#!Arrest\n"
     ]
    }
   ],
   "source": [
    "for word in docs['FBIS3-10082']['text'].strip().split(\" \"):\n",
    "    if \"$#!\" in word:\n",
    "        if word.replace(\"$#!\", \"\") in conceptsEmbeddings.docvecs.index2entity:\n",
    "            print(\"\\t\\t\\t\\t\\t\", word)\n",
    "        else:\n",
    "            print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
