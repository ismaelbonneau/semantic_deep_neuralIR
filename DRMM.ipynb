{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRMM (Deep Relevance Matching Model)\n",
    "\n",
    "_Ismaël Bonneau_\n",
    "\n",
    "\n",
    "But de ce notebook: Construire une architecture DRMM fonctionnelle avec Keras.\n",
    "\n",
    "Pour cela, 2 étapes:\n",
    "\n",
    "- construire la chaîne de pré traitements:\n",
    "    - générer des paires document-requête non pertinentes et pertinentes pour l'apprentissage\n",
    "    - générer des histogrammes d'interaction locales au niveau document-requête\n",
    "- construire l'architecture DRMM\n",
    "\n",
    "Les interractions sont pour le moment des interactions locales sur des word embeddings et sont mesurées comme une similarité cosinus entre les vecteurs des mots de la requête et ceux du document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import sep\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "embeddings_path = \"embeddings_wiki2017\"\n",
    "dataset_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré traitements: \n",
    "\n",
    "### Récupérer des word embeddings \n",
    "\n",
    "Ce word embedding a les caractéristiques suivantes:\n",
    "\n",
    "- Gensim Continuous Skipgram\n",
    "- taille de vecteur ${300}$\n",
    "- window ${5}$\n",
    "- entrainé sur wikipédia février 2017 en langue anglaise\n",
    "- lemmatisation\n",
    "- ${273992}$ mots\n",
    "\n",
    "http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(embeddings_path + sep + \"model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [w for w in model.vocab]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word', vocabulary=vocabulary, binary=True, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Montpellier', 0.7763358354568481),\n",
       " ('Rennes', 0.7535181045532227),\n",
       " ('Nantes', 0.7404719591140747),\n",
       " ('Marseille', 0.7251037359237671),\n",
       " ('nîmes', 0.7154472470283508),\n",
       " ('Narbonne', 0.7153515815734863),\n",
       " ('Béziers', 0.7148354649543762),\n",
       " ('Poitiers', 0.7118483781814575),\n",
       " ('Perpignan', 0.7109518051147461),\n",
       " ('Bordeaux', 0.7101389765739441)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"Toulouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Récupère les paires de pertinence/non pertinence pour chaque requête \n",
    "\n",
    "On génère un dictionnaire qui contient pour chaque requête en clé, un dictionnaire contenant 2 listes:\n",
    "- \"relevant\" contenant des id de document pertinents pour la requête.\n",
    "- \"irrelevant\" contenant des id de document non pertinents pour la requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paires = {}\n",
    "\n",
    "with open(dataset_path + sep + \"qrels.robust2004.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        lol = line.split()\n",
    "        paires.setdefault(lol[0], {})\n",
    "        paires[lol[0]].setdefault('relevant', []) \n",
    "        paires[lol[0]].setdefault('irrelevant', [])\n",
    "        if lol[-1] == '1':\n",
    "            paires[lol[0]][\"relevant\"].append(lol[2])\n",
    "        else:\n",
    "            paires[lol[0]][\"irrelevant\"].append(lol[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On récupère les requêtes:\n",
    "\n",
    "Elles se trouvent sous forme de tuple ([mots clés], [texte de la requête]). On ne garde que les mots clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation\n",
    "\n",
    "def clean(txt):\n",
    "    return txt.replace(\",\", \"\").replace(\".\", \"\")\n",
    "\n",
    "with open(dataset_path + sep + \"robust2004.txt\", \"r\") as f:\n",
    "    queries = ast.literal_eval(f.read())\n",
    "queries = {d:clean(queries[d][0]) for d in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international organized crime\n",
      "foreign minorities germany\n"
     ]
    }
   ],
   "source": [
    "print(queries[\"301\"])\n",
    "print(queries[\"401\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_term_maxlen(queries):\n",
    "    return np.max([len(queries[q].split()) for q in queries])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le DRMM a deux entrées: une entrée interactions et une entrée termes.\n",
    "\n",
    "L'entrée termes prend un vecteur d'idf des termes de la requête. Il faut donc pouvoir récupérer efficacement des idf. Pour cela, on construit un dictionnaire terme -> idf qui nous servira dans l'étape d'après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J'essaie le chargement des idf avec sklearn mais tu peux essayer avec elasticsearch'\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [] #balancer l'ouverture des docs ici\n",
    "vectorizer = TfidfVectorizer(\n",
    "                        use_idf=True,\n",
    "                        smooth_idf=True, \n",
    "                        sublinear_tf=False,\n",
    "                        binary=False,\n",
    "                        min_df=1, max_df=1.0, max_features=None,\n",
    "                        ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit(corpus) #osef du transform\n",
    "idf = vectorizer.idf_\n",
    "idf = dict(zip(vectorizer.get_feature_names(), idf)) #notre dictionnaire terme -> idf\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(idf, open(\"idf_robust2004.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut maintenant construire l'histogramme des interactions entre les embeddings de la requête et ceux du document.\n",
    "\n",
    "On prend comme exemple 4 bins: ${[-1, -0.5]}$ ${[-0.5, 0]}$ ${[0, 0.5]}$ ${[0.5, 1]}$.\n",
    "\n",
    "**Plusieurs manières de construire un histogramme**: compter le nombre de valeurs, compter puis normaliser..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "\n",
    "class DRMMinputGenerator:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, query_term_maxlen, intervals, normalize=False):\n",
    "        self.q_max_len = query_term_maxlen\n",
    "        self.intervals = intervals\n",
    "        self.normalize = normalize\n",
    "        self.intvlsArray = np.linspace(-1, 1, self.intervals)\n",
    "        \n",
    "    def set_params(self, model_wv, vectorizer):\n",
    "        self.model_wv = model_wv\n",
    "        self.vectorizer = vectorizer\n",
    "    \n",
    "    def get_idf_vec(self, q):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        vec = np.zeros(self.q_max_len)\n",
    "        for i, term in enumerate(q.split()):\n",
    "            if term in self.model_wv: #je suis pas sûr de mon coup là\n",
    "                vec[i] = self.idf_values[term] \n",
    "                \n",
    "        return vec\n",
    "        \n",
    "    def hist(self, query, document):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for i in query.nonzero()[1]:\n",
    "            histo = []\n",
    "            for j in document.nonzero()[1]:\n",
    "                histo.append(cosine_similarity([self.model_wv.vectors[i]], [self.model_wv.vectors[j]])[0][0])\n",
    "            histo, bin_edges = np.histogram(histo, bins=self.intvlsArray)\n",
    "            if self.normalize:\n",
    "                histo = histo / histo.sum()\n",
    "            X.append(histo)\n",
    "        if len(query.nonzero()[1]) < self.query_term_maxlen:\n",
    "            # compléter avec des zéro\n",
    "            for i in range(self.query_term_maxlen - len(query.nonzero()[1])):\n",
    "                X.append([0]*self.intervals)\n",
    "        #retourner histogramme interactions\n",
    "        return np.array(X)\n",
    "    \n",
    "    def load_data(self, queries_file, qrel_file, idf, test_size=0.2):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.idf_values = idf\n",
    "        #lecture des requêtes\n",
    "        with open(dataset_path + sep + queries_file, \"r\") as f:\n",
    "            queries = ast.literal_eval(f.read())\n",
    "            queries = {d:queries[d][0] for d in queries}\n",
    "        #spliter les requêtes en train/test\n",
    "        lol = list(queries.keys())\n",
    "        random.shuffle(lol)\n",
    "        test_keys = lol[:int(test_size * len(lol))]\n",
    "        train_keys = lol[int(test_size * len(lol)):]\n",
    "        \n",
    "        #maintenant on crée les paires\n",
    "        paires = {}\n",
    "\n",
    "        with open(dataset_path + sep + qrel_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                lol = line.split()\n",
    "                paires.setdefault(lol[0], {})\n",
    "                paires[lol[0]].setdefault('relevant', []) \n",
    "                paires[lol[0]].setdefault('irrelevant', [])\n",
    "                if lol[-1] == '1':\n",
    "                    paires[lol[0]][\"relevant\"].append(lol[2])\n",
    "                else:\n",
    "                    paires[lol[0]][\"irrelevant\"].append(lol[2])\n",
    "                    \n",
    "        #pour chaque requête on va générer autant de paires relevant que irrelevant\n",
    "        #pour nos besoins on va alterner paires positives et paires négatives\n",
    "        train_hist = [] # les histogrammes d'interraction\n",
    "        test_hist = []\n",
    "        train_idf = [] #les vecteurs d'idf\n",
    "        test_idf = []\n",
    "        \n",
    "        for requete in train_keys:\n",
    "            #recuperer les mots dont on connait les embeddings dans la query\n",
    "            q = self.vectoriser.transform(queries[requete])\n",
    "            idf_vec = self.get_idf_vec(queries[requete])\n",
    "            for pos, neg in zip(paires[requete][\"relevant\"], paires[requete][\"irrelevant\"]):\n",
    "                #lire le doc, la requete et creer l'histogramme d'interraction\n",
    "                \n",
    "                #yanis fous la partie lecture de doc ici stp\n",
    "                d = self.vectoriser.transform(\"mettre le doc positif ici stp\")\n",
    "                train_hist.append(self.hist(q, d)) #append le doc positif\n",
    "                train_idf.apprend(idf_vec) #append le vecteur idf de la requête\n",
    "                d = self.vectoriser.transform(\"mettre le doc negatif ici stp\")\n",
    "                train_hist.append(self.hist(q, d)) #append le doc négatif\n",
    "                train_idf.apprend(idf_vec) #append le vecteur idf de la requête\n",
    "        train_labels = np.zeros(len(train_hist))\n",
    "        train_labels[::2] = 1\n",
    "        \n",
    "        \n",
    "        for requete in test_keys:\n",
    "            #recuperer les mots dont on connait les embeddings dans la query\n",
    "            q = self.vectoriser.transform(queries[requete])\n",
    "            idf_vec = self.get_idf_vec(queries[requete])\n",
    "            for pos, neg in zip(paires[requete][\"relevant\"], paires[requete][\"irrelevant\"]):\n",
    "                #lire le doc, la requete et creer l'histogramme d'interraction\n",
    "                \n",
    "                #yanis fous la partie lecture de doc ici stp\n",
    "                d = self.vectoriser.transform(\"mettre le doc positif ici stp\")\n",
    "                test_hist.append(self.hist(q, d)) #append le doc positif\n",
    "                test_idf.apprend(idf_vec) #append le vecteur idf de la requête\n",
    "                d = self.vectoriser.transform(\"mettre le doc negatif ici stp\")\n",
    "                test_hist.append(self.hist(q, d)) #append le doc négatif\n",
    "                test_idf.apprend(idf_vec) #append le vecteur idf de la requête\n",
    "        test_labels = np.zeros(len(train_hist))\n",
    "        test_labels[::2] = 1\n",
    "        \n",
    "        #éventuellement sauvegarder tout ça sur le disque comme ça c fait une bonne fois pour toutes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture du modèle\n",
    "\n",
    "D'après <a href=\"https://dl.acm.org/citation.cfm?id=2983769\">l'article</a>\n",
    "\n",
    "Code d'après <a href=\"https://github.com/sebastian-hofstaetter/neural-ranking-drmm/blob/master/neural-ranking/keras_model.py\">ce génie</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Activation, Lambda, Permute, merge\n",
    "from keras.layers import Reshape, Dot\n",
    "from keras.activations import softmax\n",
    "\n",
    "\n",
    "def build_keras_model(params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer_interactions = keras.initializers.RandomUniform(minval=-0.1, maxval=0.1, seed=11)\n",
    "    initializer_gating = keras.initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=11)\n",
    "    \n",
    "    #input interactions\n",
    "    interactions = Input(name='interactions', shape=(params['query_term_maxlen'], params['hist_size']))\n",
    "    \n",
    "    #input des term vectors de la query\n",
    "    query = Input(name='term_vector', shape=(params['query_term_maxlen'],1))\n",
    "\n",
    "    #partie feed forward\n",
    "    z = interactions\n",
    "    for i in range(len(params['hidden_sizes'])):\n",
    "        z = Dense(params['hidden_sizes'][i], kernel_initializer=initializer_interactions, name=\"dense_layer_{}\".format(i))(z)\n",
    "        z = Activation('tanh', name=\"activation_of_layer_{}\".format(i))(z)\n",
    "\n",
    "    z = Permute((2, 1))(z)\n",
    "    z = Reshape((params['query_term_maxlen'],))(z)\n",
    "\n",
    "    #la partie term gating\n",
    "    q_w = Dense(1, kernel_initializer=initializer_gating, use_bias=False, name=\"gating_W\")(query)\n",
    "    q_w = Lambda(lambda x: softmax(x, axis=1), output_shape=(params['query_term_maxlen'],))(q_w)\n",
    "    q_w = Reshape((params[\"query_term_maxlen\"],))(q_w)\n",
    "\n",
    "    # combination of softmax(query term idf) and feed forward result per query term\n",
    "    out_ = Dot(axes=[1, 1], name='s')([z, q_w])\n",
    "\n",
    "    model = Model(inputs=[query, interactions], outputs=[out_])\n",
    "\n",
    "    return model\n",
    "\n",
    "# from https://github.com/faneshion/MatchZoo/blob/master/matchzoo/losses/rank_losses.py\n",
    "from keras.backend import tf\n",
    "from keras.losses import *\n",
    "from keras.layers import Lambda\n",
    "# y_true is IGNORED (!), you don't have to set a label to train (?)\n",
    "# y_pred contains the complete batch (!)\n",
    "#  -> the slicing splits the tensors in even and odd (pos and negative from the input)\n",
    "#  -> VERY IMPORTANT: The input data must not be shuffled !! shuffle = False\n",
    "\n",
    "def rank_hinge_loss(y_true, y_pred):\n",
    "\n",
    "    y_pos = Lambda(lambda a: a[::2, :], output_shape= (1,))(y_pred)\n",
    "    y_neg = Lambda(lambda a: a[1::2, :], output_shape= (1,))(y_pred)\n",
    "    \n",
    "    loss = K.maximum(0., 1. + y_neg - y_pos)\n",
    "    return K.mean(loss)\n",
    "\n",
    "#du coup pour l'entrainement dans le batch il faut aligner des paires de doc de la forme \n",
    "#positif-négatif-positif-négatif... etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "interactions (InputLayer)       (None, 4, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_0 (Dense)           (None, 4, 5)         30          interactions[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_of_layer_0 (Activati (None, 4, 5)         0           dense_layer_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 4, 32)        192         activation_of_layer_0[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_of_layer_1 (Activati (None, 4, 32)        0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 4, 1)         33          activation_of_layer_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "term_vector (InputLayer)        (None, 4, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_of_layer_2 (Activati (None, 4, 1)         0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating_W (Dense)                (None, 4, 1)         1           term_vector[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute_22 (Permute)            (None, 1, 4)         0           activation_of_layer_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 4)            0           gating_W[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_43 (Reshape)            (None, 4)            0           permute_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_44 (Reshape)            (None, 4)            0           lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "s (Dot)                         (None, 1)            0           reshape_43[0][0]                 \n",
      "                                                                 reshape_44[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 256\n",
      "Trainable params: 256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"hidden_sizes\": [5, 32, 1],\n",
    "    \"hist_size\": 5,\n",
    "    \"query_term_maxlen\": 4,\n",
    "    \"embedding_size\": 300\n",
    "}\n",
    "\n",
    "drmm = build_keras_model(params)\n",
    "drmm.compile(loss=rank_hinge_loss, optimizer='adam')\n",
    "drmm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-2d259890b1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mann_viz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Artificial Neural network - Model Visualization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ann_visualizer/visualize.py\u001b[0m in \u001b[0;36mann_viz\u001b[0;34m(model, view, filename, title)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mhidden_layers_nr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "ann_viz(drmm, title=\"Artificial Neural network - Model Visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
