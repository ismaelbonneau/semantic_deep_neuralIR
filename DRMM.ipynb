{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRMM (Deep Relevance Matching Model)\n",
    "\n",
    "\n",
    "But de ce notebook: Construire une architecture DRMM fonctionnelle avec Pytorch.\n",
    "\n",
    "Pour cela, 2 étapes:\n",
    "\n",
    "- construire la chaîne de pré traitements:\n",
    "    - générer des paires document-requête non pertinentes et pertinentes pour l'apprentissage\n",
    "    - générer des histogrammes d'interaction locales au niveau document-requête\n",
    "- construire l'architecture DRMM\n",
    "\n",
    "Les interractions sont pour le moment des interactions locales sur des word embeddings et sont mesurées comme une similarité cosinus entre les vecteurs des mots de la requête et ceux du document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import sep\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "embeddings_path = \"embeddings_wiki2017\"\n",
    "dataset_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré traitements: \n",
    "\n",
    "### Récupérer des word embeddings \n",
    "\n",
    "Ce word embedding a les caractéristiques suivantes:\n",
    "\n",
    "- Gensim Continuous Skipgram\n",
    "- taille de vecteur ${300}$\n",
    "- window ${5}$\n",
    "- entrainé sur wikipédia février 2017 en langue anglaise\n",
    "- pas de lemmatisation\n",
    "- ${302866}$ mots\n",
    "\n",
    "http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(embeddings_path + sep + \"model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [w for w in model.vocab]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', vocabulary=vocabulary, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Récupère les paires de pertinence/non pertinence pour chaque requête \n",
    "\n",
    "On génère un dictionnaire qui contient pour chaque requête en clé, un dictionnaire contenant 2 listes:\n",
    "- \"relevant\" contenant des id de document pertinents pour la requête.\n",
    "- \"irrelevant\" contenant des id de document non pertinents pour la requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paires = {}\n",
    "\n",
    "with open(dataset_path + sep + \"qrels.robust2004.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        lol = line.split()\n",
    "        paires.setdefault(lol[0], {})\n",
    "        paires[lol[0]].setdefault('relevant', []) \n",
    "        paires[lol[0]].setdefault('irrelevant', [])\n",
    "        if lol[-1] == '1':\n",
    "            paires[lol[0]][\"relevant\"].append(lol[2])\n",
    "        else:\n",
    "            paires[lol[0]][\"irrelevant\"].append(lol[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On récupère les requêtes:\n",
    "\n",
    "Elles se trouvent sous forme de tuple ([mots clés], [texte de la requête]). On ne garde que les mots clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "with open(dataset_path + sep + \"robust2004.txt\", \"r\") as f:\n",
    "    queries = ast.literal_eval(f.read())\n",
    "queries = {d:queries[d][0] for d in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international organized crime\n",
      "implant dentistry\n"
     ]
    }
   ],
   "source": [
    "print(queries[\"301\"])\n",
    "print(queries[\"308\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_term_maxlen(queries):\n",
    "    return np.max([len(queries[q].split()) for q in queries])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut maintenant construire l'histogramme des interactions entre les embeddings de la requête et ceux du document.\n",
    "\n",
    "On prend comme exemple 4 bins: ${[-1, -0.5]}$ ${[-0.5, 0]}$ ${[0, 0.5]}$ ${[0.5, 1]}$.\n",
    "\n",
    "**Plusieurs manières de construire un histogramme**: compter le nombre de valeurs, compter puis normaliser..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class DRMMinputGenerator:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, query_term_maxlen, intervals, normalize=False):\n",
    "        self.q_max_len = query_term_maxlen\n",
    "        self.intervals = intervals\n",
    "        self.normalize = normalize\n",
    "        self.intvlsArray = np.linspace(-1, 1, self.intervals)\n",
    "        \n",
    "    def set_params(self, model_wv, vectorizer):\n",
    "        self.model_wv = model_wv\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "    def hist(self, query, document):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for i in query.nonzero()[1]:\n",
    "            histo = []\n",
    "            for j in document.nonzero()[1]:\n",
    "                histo.append(cosine_similarity([self.model_wv.vectors[i]], [self.model_wv.vectors[j]])[0][0])\n",
    "            histo, bin_edges = np.histogram(histo, bins=self.intvlsArray)\n",
    "            if self.normalize:\n",
    "                histo = histo / histo.sum()\n",
    "            X.append(histo)\n",
    "        #retourner histogramme interactions + embeddings termes query\n",
    "        return np.array(X), np.array([self.model_wv.vectors[i] for i in query.nonzero()[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture du modèle\n",
    "\n",
    "D'après <a href=\"https://dl.acm.org/citation.cfm?id=2983769\">l'article</a>\n",
    "\n",
    "Code d'après <a href=\"https://github.com/sebastian-hofstaetter/neural-ranking-drmm/blob/master/neural-ranking/keras_model.py\">ce génie</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Activation, Lambda, Permute, merge\n",
    "from keras.layers import Reshape, Dot\n",
    "from keras.activations import softmax\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def build_keras_model(params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer_interactions = keras.initializers.RandomUniform(minval=-0.1, maxval=0.1, seed=11)\n",
    "    initializer_gating = keras.initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=11)\n",
    "    \n",
    "    #input interactions\n",
    "    interactions = Input(name='interactions', shape=(params['query_term_maxlen'], params['hist_size']))\n",
    "    \n",
    "    #input des term vectors de la query\n",
    "    query = Input(name='term_vector', shape=(params['query_term_maxlen'],1))\n",
    "\n",
    "    #partie feed forward\n",
    "    z = interactions\n",
    "    for i in range(len(params['hidden_sizes'])):\n",
    "        z = Dense(params['hidden_sizes'][i], kernel_initializer=initializer_interactions, name=\"dense_layer_{}\".format(i))(z)\n",
    "        z = Activation('tanh', name=\"activation_of_layer_{}\".format(i))(z)\n",
    "\n",
    "    z = Permute((2, 1))(z)\n",
    "    z = Reshape((params['query_term_maxlen'],))(z)\n",
    "\n",
    "    #la partie term gating\n",
    "    q_w = Dense(1, kernel_initializer=initializer_gating, use_bias=False, name=\"gating_W\")(query)\n",
    "    q_w = Lambda(lambda x: softmax(x, axis=1), output_shape=(params['query_term_maxlen'],))(q_w)\n",
    "    q_w = Reshape((params[\"query_term_maxlen\"],))(q_w)\n",
    "\n",
    "    # combination of softmax(query term idf) and feed forward result per query term\n",
    "    out_ = Dot(axes=[1, 1], name='s')([z, q_w])\n",
    "\n",
    "    model = Model(inputs=[query, interactions], outputs=[out_])\n",
    "\n",
    "    return model\n",
    "\n",
    "# from https://github.com/faneshion/MatchZoo/blob/master/matchzoo/losses/rank_losses.py\n",
    "from keras.backend import tf\n",
    "from keras.losses import *\n",
    "from keras.layers import Lambda\n",
    "# y_true is IGNORED (!), you don't have to set a label to train (?)\n",
    "# y_pred contains the complete batch (!)\n",
    "#  -> the slicing splits the tensors in even and odd (pos and negative from the input)\n",
    "#  -> VERY IMPORTANT: The input data must not be shuffled !! shuffle = False\n",
    "\n",
    "def rank_hinge_loss(y_true, y_pred):\n",
    "\n",
    "    y_pos = Lambda(lambda a: a[::2, :], output_shape= (1,))(y_pred)\n",
    "    y_neg = Lambda(lambda a: a[1::2, :], output_shape= (1,))(y_pred)\n",
    "    \n",
    "    loss = K.maximum(0., 1. + y_neg - y_pos)\n",
    "    return K.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "interactions (InputLayer)       (None, 4, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_0 (Dense)           (None, 4, 5)         30          interactions[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_of_layer_0 (Activati (None, 4, 5)         0           dense_layer_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 4, 32)        192         activation_of_layer_0[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_of_layer_1 (Activati (None, 4, 32)        0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 4, 1)         33          activation_of_layer_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "term_vector (InputLayer)        (None, 4, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_of_layer_2 (Activati (None, 4, 1)         0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating_W (Dense)                (None, 4, 1)         1           term_vector[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute_18 (Permute)            (None, 1, 4)         0           activation_of_layer_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 4)            0           gating_W[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_35 (Reshape)            (None, 4)            0           permute_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_36 (Reshape)            (None, 4)            0           lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "s (Dot)                         (None, 1)            0           reshape_35[0][0]                 \n",
      "                                                                 reshape_36[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 256\n",
      "Trainable params: 256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"hidden_sizes\": [5, 32, 1],\n",
    "    \"hist_size\": 5,\n",
    "    \"query_term_maxlen\": 4,\n",
    "    \"embedding_size\": 300\n",
    "}\n",
    "\n",
    "model = build_keras_model(params)\n",
    "model.summary()\n",
    "#model.compile(loss=rank_hinge_loss, optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
