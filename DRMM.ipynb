{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRMM (Deep Relevance Matching Model)\n",
    "\n",
    "_Ismaël Bonneau_\n",
    "\n",
    "Mots-clés: _Relevance Matching_, _Semantic Matching_, _Neural Models_,\n",
    "_Ad-hoc Retrieval_, _Ranking Models_\n",
    "\n",
    "\n",
    "#### But de ce notebook: Comprendre et construire une architecture **DRMM** fonctionnelle avec **pytorch**, et l'expliquer de façon concise.\n",
    "\n",
    "Un gros bisou à Daniel Godoy pour son tutoriel <a href=\"https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\">comprendre pytorch par l'exemple pas à pas</a> (en Anglais).\n",
    "\n",
    "Pour cela, 2 étapes:\n",
    "\n",
    "- construire la chaîne de pré traitements:\n",
    "    - générer des paires document-requête non pertinentes et pertinentes pour l'apprentissage\n",
    "    - générer des histogrammes d'interaction locales au niveau document-requête\n",
    "- construire l'architecture DRMM\n",
    "\n",
    "Les interractions sont pour le moment des interactions locales sur des word embeddings et sont mesurées comme une similarité cosinus entre les vecteurs des mots de la requête et ceux du document.\n",
    "\n",
    "## En quoi consiste le modèle DRMM?\n",
    "\n",
    "lien vers <a href=\"https://arxiv.org/pdf/1711.08611.pdf\">l'article</a> (en Anglais) par _Jiafeng Guo_, _Yixing Fan_, _Qingyao Ai_, _W. Bruce Croft_ (2017) [1]\n",
    "\n",
    "DRMM (Deep Relevance Matching Model) est un modèle de réseau de neurones profond pour la RI (recherche d'information).\n",
    "\n",
    "Un des objectifs principaux de la RI est de déterminer la **pertinence** d'un document (cela peut être un document court sous forme d'un paragraphe, ou long, voire très long) par rapport à une requête donnée. Un moteur de RI traditionnel retournera alors une liste ordonnée des documents par pertinence par rapport à une requête posée par l'utilisateur, avec en tête de liste les documents les plus pertinents.\n",
    "\n",
    "Un problème qui se pose avec de nombreux modèles de RI est . Certains termes de la requête ne se trouvent pas toujours dans des documents qui sont pourtant pertinents pour cette requête. Pensons à une requête sur la pigeons dans la ville de Paris: Un document qui aurait pour sujet le \"problème envahissant des oiseaux dans la capitale Française\", sans contenir une seule fois les mots Pigeons et Paris, aurait peu de chance d'être considéré comme pertinent.\n",
    "\n",
    "Plusieurs solutions sont donc proposées pour résoudre ce problème.\n",
    "\n",
    "DRMM est un modèle **orienté interactions**, qui applique une fonction apprise (un réseau de neurones n'est rien d'autres qu'une fonction très complexe) à des interactions entre un document et une requête, qui ne font donc pas partie du réseau et ne sont pas apprises. Cette fonction a pour but de calculer un **score pour la paire document-requête**, score d'autant **plus élevé que le document est pertinent pour la requête**.\n",
    "\n",
    "Il se compose de deux parties: \n",
    "\n",
    "- Une partie **\"feed forward matching network\"** qui est un simple perceptron multicouche. Il s'agit dans l'implémentation de [1] d'un perceptron à 3 couches, de taille 30 neurones, 5 neurones, et 1 neurone. Cette partie vient calculer un score pour l'interaction de chaque terme de la requête avec l'ensemble des termes du document. Pour une requête de 5 termes, le partie feed forward matching produira donc un vecteur de dimension 5, pour 1 score par terme de la requête. On a donc un bloc qui prend en entrée une matrice d'interactions des termes de la requête, et qui rend en sortie un vecteur de score pour chaque terme de la requête.\n",
    "\n",
    "- Une partie **\"term gating network\"** qui est un perceptron à une couche. Il s'agit uniquement d'apprendre un vecteur qui vient mutiplier chaque \"vecteur de terme\" de la requête et ensuitepondérer les scores calculés par la partie feed forward. \n",
    "\n",
    "<img src=\"images/DRMMschema.png\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import sep\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "embeddings_path = \"embeddings_wiki2017\"\n",
    "dataset_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré traitements: \n",
    "\n",
    "### Récupérer des word embeddings \n",
    "\n",
    "Ce word embedding a les caractéristiques suivantes:\n",
    "\n",
    "- gensim Continuous Skipgram\n",
    "- taille de vecteur ${300}$\n",
    "- window ${5}$\n",
    "- entrainé sur le corpus gigaword 5th edition + wikipédia février 2017\n",
    "- lemmatisation\n",
    "- ${259882}$ mots\n",
    "\n",
    "http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('embeddings_wiki2017/model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MARSEILLE', 0.6269462704658508),\n",
       " ('toulouse', 0.5996948480606079),\n",
       " ('PERPIGNAN', 0.596558690071106),\n",
       " ('MARCOUSSIS', 0.5767359733581543),\n",
       " ('TOULON', 0.5707815289497375),\n",
       " ('madrid', 0.5648114681243896),\n",
       " ('edinburgh', 0.5617228746414185),\n",
       " ('MONTPELLIER', 0.560294508934021),\n",
       " ('london', 0.5577080845832825),\n",
       " ('BREST', 0.5576730370521545)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(\"marseille\") # ti as compris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "On utilise pour cet exemple le dataset <a href=\"https://trec.nist.gov/data/robust/04.guidelines.html\">robust 2004</a>. Il s'agit d'une collection de documents datés de 2004 provenant de journaux américains, le LA times, le Financial times, le Federal Register 94, et le Foreign Broadcast Information Service.\n",
    "\n",
    "\n",
    "| nombre de requêtes      |  jugements de pertinence    |\n",
    "| -------------:|  ------------- |\n",
    "|     **250**       |   **311,410**\n",
    "\n",
    "| collection      |     nombre de documents    |     taille en Mo    |\n",
    "| ------------- |: -------------: |: -------------: |\n",
    "| Financial times      |        210 158        |        564        |\n",
    "| La times       |        131,896        |        475        |\n",
    "| FBIS      |        130,471        |        470        |\n",
    "| FR94      |        55,630        |        395        |\n",
    "| **TOTAL**      |        **528,155**        |        **1904**        |\n",
    "\n",
    "\n",
    "### On Récupère les paires de pertinence/non pertinence pour chaque requête \n",
    "\n",
    "On génère un dictionnaire qui contient pour chaque requête en clé, un dictionnaire contenant 2 listes:\n",
    "- \"relevant\" contenant des id de document pertinents pour la requête.\n",
    "- \"irrelevant\" contenant des id de document non pertinents pour la requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paires = {}\n",
    "\n",
    "with open(dataset_path + sep + \"qrels.robust2004.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        lol = line.split()\n",
    "        paires.setdefault(lol[0], {})\n",
    "        paires[lol[0]].setdefault('relevant', []) \n",
    "        paires[lol[0]].setdefault('irrelevant', [])\n",
    "        if lol[-1] == '1':\n",
    "            paires[lol[0]][\"relevant\"].append(lol[2])\n",
    "        else:\n",
    "            paires[lol[0]][\"irrelevant\"].append(lol[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On récupère les requêtes:\n",
    "\n",
    "Elles se trouvent sous forme de tuple ([mots clés], [texte de la requête]). On ne garde que les mots clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation\n",
    "from krovetzstemmer import Stemmer #stemmer pas mal pour la PR\n",
    "ks = Stemmer()\n",
    "\n",
    "def clean(txt):\n",
    "    return \" \".join([ks.stem(t) for t in txt.replace(\",\", \"\").replace(\".\", \"\").split(\" \")])\n",
    "     \n",
    "with open(dataset_path + sep + \"robust2004.txt\", \"r\") as f:\n",
    "    queries = ast.literal_eval(f.read())\n",
    "queries = {d:clean(queries[d][0]) for d in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international organized crime\n",
      "foreign minority germany\n"
     ]
    }
   ],
   "source": [
    "print(queries[\"301\"])\n",
    "print(queries[\"401\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le DRMM a deux entrées: une entrée interactions et une entrée termes.\n",
    "\n",
    "L'entrée termes prend un vecteur d'idf des termes de la requête. Il faut donc pouvoir récupérer efficacement des idf. Pour cela, on construit un dictionnaire terme -> idf qui nous servira dans l'étape d'après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf marseille:  8.366004087237275\n"
     ]
    }
   ],
   "source": [
    "#bon là on charge du coup vu que le fichier est sauvegardé sur le disque\n",
    "idf = pickle.load(open(\"idf_robust2004.pkl\", \"rb\"))\n",
    "print(\"idf marseille: \", idf[\"marseille\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On prépare dans des fichiers les matrices d'interactions et les vecteurs d'idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query chargé\n",
      "relevance chargé\n",
      "docs chargés\n"
     ]
    }
   ],
   "source": [
    "from datasets import Robust04 #permet de gérer le chargement et le traitement de robust2004\n",
    "\n",
    "inputgenerator = Robust04(intervals=30, model_wv=word_vectors)\n",
    "inputgenerator.load_idf(idf_file=\"idf_robust2004.pkl\")\n",
    "inputgenerator.load_all_query(file_query=\"data/robust2004.txt\")\n",
    "inputgenerator.load_relevance(file_rel=\"data/qrels.robust2004.txt\")\n",
    "inputgenerator.load_all_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette méthode calcule les matrices d'interraction et les charge dans des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de requetes: 249.\n",
      "data completed\n"
     ]
    }
   ],
   "source": [
    "inputgenerator.prepare_data_forNN(\"saved_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette méthode pré calcule les matrices d'interraction des résultats de BM25 et les charge dans des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de requetes: 249.\n",
      "data completed\n"
     ]
    }
   ],
   "source": [
    "inputgenerator.prepare_data_reranking(pickle.load(open(\"results_bm25_robust.pkl\", \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On peut maintenant récupérer les matrices d'interraction pour les utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 requetes en train, 47 en test.\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2 # 20% de requêtes en test\n",
    "\n",
    "#random.seed(1401)\n",
    "\n",
    "query_idf = pickle.load(open(\"saved_data/query_idf.pkl\", \"rb\"))\n",
    "\n",
    "interractions_train_pos = []\n",
    "interractions_train_neg = []\n",
    "interractions_test_pos = []\n",
    "interractions_test_neg = []\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for id_requete in paires:\n",
    "    if id_requete != '634':\n",
    "        saintjeanlapuenta = np.load(\"saved_data/{}_interractions.npy\".format(id_requete))\n",
    "        if random.random() < test_size:\n",
    "            test.append(id_requete)\n",
    "            for neg, pos in zip(saintjeanlapuenta[::2], saintjeanlapuenta[1::2]):\n",
    "                interractions_test_pos.append([torch.from_numpy(pos).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "                interractions_test_neg.append([torch.from_numpy(neg).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "        else:\n",
    "            train.append(id_requete)\n",
    "            for neg, pos in zip(saintjeanlapuenta[::2], saintjeanlapuenta[1::2]):\n",
    "                interractions_train_pos.append([torch.from_numpy(pos).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "                interractions_train_neg.append([torch.from_numpy(neg).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "\n",
    "print(\"{} requetes en train, {} en test.\".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture du modèle\n",
    "\n",
    "### Avec pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MarginRankingLoss\n",
    "\n",
    "#hérite de la classe Pytorch Module\n",
    "class DRMM(torch.nn.Module):\n",
    "    def __init__(self, hist_size, query_term_maxlen, hidden_sizes=[5,1], use_cuda=True):\n",
    "        \n",
    "        if hidden_sizes[-1] != 1:\n",
    "            raise ValueError(\"la taille du dernier layer doit être 1\")\n",
    "            \n",
    "        super(DRMM, self).__init__()\n",
    "        self.MLP0 = torch.nn.Linear(hist_size, hidden_sizes[0])\n",
    "        torch.nn.init.xavier_normal_(self.MLP0.weight, gain = torch.nn.init.calculate_gain('tanh'))\n",
    "        self.MLP1 = torch.nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        torch.nn.init.xavier_normal_(self.MLP1.weight, gain = torch.nn.init.calculate_gain('tanh'))\n",
    "            \n",
    "        #initialisation du vecteur de term gating\n",
    "        self.termgating = torch.nn.Linear(query_term_maxlen, query_term_maxlen, bias=False)\n",
    "        torch.nn.init.xavier_normal_(self.termgating.weight, gain = torch.nn.init.calculate_gain('tanh'))\n",
    "        \n",
    "    #méthode forward à redéfinir\n",
    "    def forward(self, interractions, termvector):\n",
    "        \"\"\"\n",
    "        interractions: (query_term_maxlen, hist_size)\n",
    "        termvector: \n",
    "        \"\"\"\n",
    "        #partie histogramme\n",
    "        interractions_output = self.MLP0(interractions)\n",
    "        interractions_output = torch.nn.functional.tanh(interractions_output)\n",
    "        interractions_output = self.MLP1(interractions_output)\n",
    "        interractions_output = torch.nn.functional.tanh(interractions_output)\n",
    "        interractions_output = interractions_output.squeeze() #passe de (query_term_maxlen, 1) à (1, query_term_maxlen)\n",
    "        #partie term gating \n",
    "        gating_output = torch.nn.functional.softmax(self.termgating(termvector)).squeeze()\n",
    "        #combiner les 2 avec un produit scalaire\n",
    "        axis = 0\n",
    "        #si on travaille par mini-batch:\n",
    "        if len(interractions.size()) == 3:\n",
    "            axis = 1\n",
    "        s = torch.sum(interractions_output * gating_output, dim = axis) #dim = 0 ne marche qui pas batch.\n",
    "        return s\n",
    "    \n",
    "    def get_model_size(self):\n",
    "        \"\"\"\n",
    "        retourne le nombre de paramètres du modèle.\n",
    "        \"\"\"\n",
    "        return sum([p.size(0) if len(p.size()) == 1 else p.size(0)*p.size(1) for p in self.parameters()])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"DRMM with {} parameters. ti as compris\".format(self.get_model_size())\n",
    "    \n",
    "    \n",
    "class HingeLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Hinge Loss\n",
    "          max(0, 1-x+y)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        output = 1-x+y\n",
    "        return output.clamp(min=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On va créer une classe de Dataset qui contient les paires pertinentes et les paires non pertinentes.\n",
    "\n",
    "Plus précisément, la classe Dataset de Pytorch est un wrapper pour X (traditionnellement les vecteurs de features) et Y (traditionnellement les labels). Ici, on a pas besoin de vecteur de labels. On peut donc transformer l'utilisation de la classe pour mettre dans X une matrice de paires (interractions relevant, vecteur de termes relevant) et dans Y la même matrice, pour les paires non pertinentes. L'avantage est que lors du shuffle avec le DataLoader, on conservera l'alignement doc pertinent/non pertinent pour une requête et on ne sera pas embêté pour utiliser la ranking hinge loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DrmmDataset(Dataset):\n",
    "    def __init__(self, pos_tensor, neg_tensor):\n",
    "        self.x = pos_tensor\n",
    "        self.y = neg_tensor\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'étape de création du DataLoader:\n",
    "\n",
    "Cette classe va nous permettre de gérer le découpage en mini batches et le shuffle. C'est un wrapper pour la classe DataSet qui fournit entre autres un itérateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DrmmDataset(interractions_train_pos, \n",
    "                            interractions_train_neg)\n",
    "val_dataset = DrmmDataset(interractions_test_pos,\n",
    "                          interractions_test_neg)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batchsize = 20\n",
    "#classe utile pour gérer les mini batches et le shuffle (crucial!)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batchsize, shuffle=True)\n",
    "#shuffle = False pour le DataLoader de test!\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On regroupe toutes les étapes de forward/backprop sur un mini-batch dans une fonction:\n",
    "\n",
    "La fonction prend deux mini batch en entrée et:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drmm_make_train_step(model, loss_fn, optimizer):\n",
    "    def drmm_train_step(pos_batch, neg_batch):\n",
    "        #mettre le modèle en mode train\n",
    "        model.train()\n",
    "        #étape forward...\n",
    "        pos_score = model(pos_batch[0], pos_batch[1])\n",
    "        neg_score = model(neg_batch[0], neg_batch[1])\n",
    "        #calcul de la loss\n",
    "        loss = loss_fn(pos_score, neg_score, torch.Tensor([1] * pos_batch[0].size()[0]))\n",
    "        #loss = loss_fn(pos_score, neg_score)\n",
    "        #calcul des gradients\n",
    "        loss.backward()\n",
    "        #mise à jour des paramètres\n",
    "        optimizer.step()\n",
    "        #reset des gradients après le passage sur ce batch\n",
    "        optimizer.zero_grad()\n",
    "        #retourner la loss\n",
    "        return loss.item() #.item()\n",
    "    \n",
    "    return drmm_train_step\n",
    "\n",
    "def drmm_make_val_step(model, loss_fn, optimizer):\n",
    "    def drmm_val_step(pos_batch, neg_batch):\n",
    "        #mettre le modèle en mode test\n",
    "        model.eval()\n",
    "        #étape forward...\n",
    "        pos_score = model(pos_batch[0], pos_batch[1])\n",
    "        neg_score = model(neg_batch[0], neg_batch[1])\n",
    "        \n",
    "        #loss = loss_fn(pos_score, neg_score, torch.Tensor([1] * pos_batch[0].size()[0]))\n",
    "        loss = loss_fn(pos_score, neg_score)\n",
    "        \n",
    "        return loss.item()\n",
    "    return drmm_val_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Et... l'apprentissage du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRMM with 177 parameters. ti as compris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**epoch 0: loss 0.9475844737038818**\n",
      "**epoch 10: loss 0.9037719822008938**\n",
      "**epoch 20: loss 0.8883330144202742**\n",
      "**epoch 30: loss 0.8860278432994603**\n",
      "**epoch 40: loss 0.8858829130331473**\n",
      "**epoch 50: loss 0.8858814978064732**\n",
      "**epoch 60: loss 0.8858242806597021**\n",
      "**epoch 70: loss 0.8846447222578626**\n",
      "**epoch 80: loss 0.8852881465279081**\n",
      "**epoch 90: loss 0.884064087234466**\n",
      "**epoch 100: loss 0.884538531379374**\n",
      "**epoch 110: loss 0.8839560149124985**\n",
      "**epoch 120: loss 0.8835946954606353**\n",
      "**epoch 130: loss 0.8838502600154736**\n",
      "**epoch 140: loss 0.8839142157537366**\n",
      "**epoch 150: loss 0.883808256821337**\n",
      "**epoch 160: loss 0.8838501332093647**\n",
      "**epoch 170: loss 0.8838341141458133**\n",
      "**epoch 180: loss 0.883943236834414**\n",
      "**epoch 190: loss 0.8839924589061567**\n",
      "**epoch 200: loss 0.8838113566730936**\n",
      "**epoch 210: loss 0.8837998941362193**\n",
      "**epoch 220: loss 0.8832308579366365**\n",
      "**epoch 230: loss 0.8831739333433792**\n"
     ]
    }
   ],
   "source": [
    "drmm = DRMM(30, 4, hidden_sizes=[5, 1])\n",
    "print(drmm)\n",
    "\n",
    "hingeloss = MarginRankingLoss(margin=1.)\n",
    "optimizer = torch.optim.Adam(drmm.parameters(), lr=0.001)\n",
    "\n",
    "make_train_step = drmm_make_train_step(drmm, hingeloss, optimizer)\n",
    "make_val_step = drmm_make_val_step(drmm, hingeloss, optimizer)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "metrics = []\n",
    "val_metrics = []\n",
    "nb_epochs = 1000\n",
    "\n",
    "bm25_dict = pickle.load(open(\"results_bm25_robust.pkl\", \"rb\"))\n",
    "reranker = Reranker(bm25_dict)\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    #parcourir tous les batches du dataloader\n",
    "    tiacompris = []\n",
    "    for pos_batch, neg_batch in train_loader:\n",
    "\n",
    "        #pos_batch[0]: les interractions positives\n",
    "        #pos_batch[1]: les query term\n",
    "        #neg_batch[0]: les interractions négatives correspondantes...\n",
    "        #neg_batch[1]: les query term correspondantes...\n",
    "        \n",
    "        #évaluation empirical loss + backward pass\n",
    "        tiacompris.append(make_train_step(pos_batch, neg_batch))\n",
    "    losses.append(np.array(tiacompris).mean())\n",
    "    \n",
    "    #évaluation RI\n",
    "    #reranker.set_model(drmm)\n",
    "    #lol = reranker.rerank(queries = train)\n",
    "    #reranker.save_results(lol, \"tiacompris\")\n",
    "    #treceval = compute_trec_eval(\"data/qrels.robust2004.txt\", \"tiacompris\")\n",
    "    #metrics.append((treceval[\"map\"], treceval[\"P_20\"]))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"**epoch {}: loss {}**\".format(i, losses[-1]))\n",
    "        #print(\"**epoch {}: loss {} map {} P_20 {}**\".format(i, losses[-1], treceval[\"map\"], treceval[\"P_20\"]))\n",
    "    \n",
    "    #on met le modèle en mode \"ne calcule pas le gradient\"\n",
    "    #with torch.no_grad():\n",
    "        #on parcourt\n",
    "        #for pos_batch_val, neg_batch_val in val_loader:    \n",
    "            #val_loss = make_val_step(pos_batch_val, neg_batch_val)\n",
    "        #val_losses.append(val_loss)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(list(range(nb_epochs)), losses, label=\"train\")\n",
    "#plt.plot(list(range(nb_epochs)), val_losses, label=\"validation\")\n",
    "plt.title(\"DRRM loss sur robust-2004\", fontsize=16)\n",
    "plt.ylabel(\"loss\", fontsize=13)\n",
    "plt.xlabel(\"epochs\", fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut maintenant tester le modèle avec des métriques de RI:\n",
    "\n",
    "Pour chacune des requêtes de test, on récupère les 2000 premiers documents retournés par *BM25*. Nous procédons en réordonnant cette liste de documents avec notre modèle *DRMM* appris, et nous calculons des performances sur les 1000 premiers documents de ce re ranking sur un ensemble de métriques de RI (**NDCG@20**, **MAP**, **P@20**), en le comparant avec le ranking original *BM25*.\n",
    "\n",
    "Pour BM25, l'indexation a été faite comme suit:\n",
    "\n",
    "- indexation sur le texte du document et non le titre\n",
    "- texte sans stopwords, ponctuation, balises\n",
    "- stemming avec Krovetz\n",
    "\n",
    "<img src=\"images/learning_to_rank.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reranker:\n",
    "    def __init__(self, bm25_dict):\n",
    "        self.bm25_dict = bm25_dict #le dictionnaire query -> 2000 docs relevants pour bm25\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def rerank(self, queries=None):\n",
    "        \n",
    "        #queries: une liste d'ID de requete pour lesquels on veut réordonner les résultats\n",
    "        if queries == None:\n",
    "            queries_to_rerank = list(self.bm25_dict.keys())\n",
    "        else:\n",
    "            queries_to_rerank = queries\n",
    "        \n",
    "        #pour chaque requete, on va charger réordonner ses résultats avec le modèle\n",
    "        query_idf = pickle.load(open(\"saved_data/query_idf.pkl\", \"rb\"))\n",
    "        reranked_dict = {}\n",
    "        for id_requete in queries_to_rerank:\n",
    "            if id_requete != \"634\":\n",
    "                with torch.no_grad():\n",
    "                    #contient une matrice (2000, query_max_len, hist_size)\n",
    "                    saintjeanlapuenta = np.load(\"data/bm25_robust/{}_interractions.npy\".format(id_requete))\n",
    "                    a = torch.from_numpy(np.tile(np.array([query_idf[id_requete]]), (saintjeanlapuenta.shape[0],1))).float()\n",
    "\n",
    "                    model_scores = self.model(torch.from_numpy(saintjeanlapuenta).float(), a).data.numpy()\n",
    "                    lol = np.argsort(model_scores)[::-1] #tri décroissant\n",
    "\n",
    "                    # reranked: liste de tuples (document_id, score)\n",
    "                    reranked_dict[id_requete] = [(self.bm25_dict[id_requete][i][0], model_scores[i]) for i in lol]\n",
    "\n",
    "        return reranked_dict\n",
    "    \n",
    "    def get_results(self, id_requete, rank_list):\n",
    "        results = []\n",
    "        for i, (doc_id, score) in enumerate(rank_list[:1000]):\n",
    "            results.append(\" \".join([id_requete, \"Q0\", doc_id, str(i + 1), str(score), \"EARIA\"]))\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def save_results(self, rank_dict, res_file):\n",
    "        \"\"\"\n",
    "        sauver sur un fichier au format attendu par TREC\n",
    "        un dictionnaire query_id -> list (doc_id, score)\n",
    "        \"\"\"\n",
    "        results = [f\"{id_requete} Q0 EMPTY 1001 -100000 EARIA\" for id_requete in rank_dict]\n",
    "        for id_requete in rank_dict:\n",
    "            results.extend(self.get_results(id_requete, rank_dict[id_requete]))\n",
    "        \n",
    "        with open(res_file, \"w\") as tiacompris:\n",
    "            tiacompris.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def compute_trec_eval(qrel_file_path, resp_file_name):\n",
    "    command = [\"trec_eval/trec_eval\" , \"-c\", \"-M1001\", \"-m\", \"map\",\"-m\", \"P.20\", qrel_file_path, resp_file_name]\n",
    "    #Le fichier trec_eval est à récuperer ici https://github.com/usnistgov/trec_eval\n",
    "    # une fois clone, il faut faire un make dans le dossier et autoriser à l'execution (chmod +x)\n",
    "    completed_process = subprocess.run(command, capture_output=True)\n",
    "    results = completed_process.stdout.decode(\"utf-8\")\n",
    "    \n",
    "    with open(\"caca\", \"w\") as f:\n",
    "        f.write(results)\n",
    "    \n",
    "    total_score = {}\n",
    "    for tkt in results.split(\"\\n\")[:2]:\n",
    "        total_score[tkt.split(\"\\t\")[0].strip()] = float(tkt.split(\"\\t\")[2].strip())\n",
    "    \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(id_requete, rank_list):\n",
    "    results = []\n",
    "    for i, (doc_id, score) in enumerate(rank_list[:1000]):\n",
    "        results.append(\" \".join([id_requete, \"Q0\", doc_id, str(i + 1), str(score), \"EARIA\"]))\n",
    "    return results\n",
    "            \n",
    "    \n",
    "def save_results(rank_dict, res_file, queries):\n",
    "    \"\"\"\n",
    "    sauver sur un fichier au format attendu par TREC\n",
    "    un dictionnaire query_id -> list (doc_id, score)\n",
    "    \"\"\"\n",
    "    results = [f\"{id_requete} Q0 EMPTY 1001 -100000 EARIA\" for id_requete in queries]\n",
    "    for id_requete in queries:\n",
    "        results.extend(get_results(id_requete, rank_dict[id_requete]))\n",
    "        \n",
    "    with open(res_file, \"w\") as tiacompris:\n",
    "        tiacompris.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP et précision avec bm25 seul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': 0.2001, 'P_20': 0.2884}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_dict = pickle.load(open(\"results_bm25_robust.pkl\", \"rb\"))\n",
    "save_results(bm25_dict, \"proutpipi\", queries=train)\n",
    "compute_trec_eval(\"data/qrels.robust2004.txt\", \"proutpipi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP et précision sur le re ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map': 0.02, 'P_20': 0.0309}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_dict = pickle.load(open(\"results_bm25_robust.pkl\", \"rb\"))\n",
    "reranker = Reranker(bm25_dict)\n",
    "reranker.set_model(drmm)\n",
    "lol = reranker.rerank(queries = train)\n",
    "reranker.save_results(lol, \"tiacompris\")\n",
    "compute_trec_eval(\"data/qrels.robust2004.txt\", \"tiacompris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perf en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map': 0.0033, 'P_20': 0.0064}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = reranker.rerank(queries = test)\n",
    "reranker.save_results(lol, \"stfe\")\n",
    "compute_trec_eval(\"data/qrels.robust2004.txt\", \"stfe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographie\n",
    "\n",
    "\n",
    "[1] <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.161&rep=rep1&type=pdf\">What Works Better for Question Answering: Stemming or\n",
    "Morphological Query Expansion?</a>\n",
    "\n",
    "[2] <a href=\"https://dl.acm.org/citation.cfm?id=160718\" >Viewing morphology as an inference process</a>\n",
    "\n",
    "[3] <a href=\"https://arxiv.org/pdf/1809.01682.pdf\">Deep Relevance Ranking Using Enhanced Document-Query Interactions</a>\n",
    "\n",
    "[4] <a href=\"http://delivery.acm.org/10.1145/2810000/2806475/p1411-kenter.pdf?ip=132.227.125.83&id=2806475&acc=ACTIVE%20SERVICE&key=7EBF6E77E86B478F%2EA72B4D473219EA0C%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1562588994_ca050bde6ad37b2c0d433db2e5df63ba\">Short Text Similarity with Word Embeddings</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
