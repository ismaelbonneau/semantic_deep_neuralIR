{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRMM (Deep Relevance Matching Model)\n",
    "\n",
    "\n",
    "But de ce notebook: Construire une architecture DRMM fonctionnelle avec Pytorch.\n",
    "\n",
    "Pour cela, 2 étapes:\n",
    "\n",
    "- construire la chaîne de pré traitements:\n",
    "    - générer des paires document-requête non pertinentes et pertinentes pour l'apprentissage\n",
    "    - générer des histogrammes d'interaction locales au niveau document-requête\n",
    "- construire l'architecture DRMM\n",
    "\n",
    "Les interractions sont pour le moment des interactions locales sur des word embeddings et sont mesurées comme une similarité cosinus entre les vecteurs des mots de la requête et ceux du document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import sep\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "embeddings_path = \"embeddings_wiki2017\"\n",
    "dataset_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré traitements: \n",
    "\n",
    "### Récupérer des word embeddings \n",
    "\n",
    "Ce word embedding a les caractéristiques suivantes:\n",
    "\n",
    "- Gensim Continuous Skipgram\n",
    "- taille de vecteur ${300}$\n",
    "- window ${5}$\n",
    "- entrainé sur wikipédia février 2017 en langue anglaise\n",
    "- pas de lemmatisation\n",
    "- ${302866}$ mots\n",
    "\n",
    "http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(embeddings_path + sep + \"model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [w for w in model.vocab]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', vocabulary=vocabulary, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Récupère les paires de pertinence/non pertinence pour chaque requête \n",
    "\n",
    "On génère un dictionnaire qui contient pour chaque requête en clé, un dictionnaire contenant 2 listes:\n",
    "- \"relevant\" contenant des id de document pertinents pour la requête.\n",
    "- \"irrelevant\" contenant des id de document non pertinents pour la requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paires = {}\n",
    "\n",
    "with open(dataset_path + sep + \"qrels.robust2004.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        lol = line.split()\n",
    "        paires.setdefault(lol[0], {})\n",
    "        paires[lol[0]].setdefault('relevant', []) \n",
    "        paires[lol[0]].setdefault('irrelevant', [])\n",
    "        if lol[-1] == '1':\n",
    "            paires[lol[0]][\"relevant\"].append(lol[2])\n",
    "        else:\n",
    "            paires[lol[0]][\"irrelevant\"].append(lol[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On récupère les requêtes:\n",
    "\n",
    "Elles se trouvent sous forme de tuple ([mots clés], [texte de la requête]). On ne garde que les mots clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "with open(dataset_path + sep + \"robust2004.txt\", \"r\") as f:\n",
    "    queries = ast.literal_eval(f.read())\n",
    "queries = {d:queries[d][0] for d in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international organized crime\n",
      "implant dentistry\n"
     ]
    }
   ],
   "source": [
    "print(queries[\"301\"])\n",
    "print(queries[\"308\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_term_maxlen(queries):\n",
    "    return np.max([len(queries[q].split()) for q in queries])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut maintenant construire l'histogramme des interactions entre les embeddings de la requête et ceux du document.\n",
    "\n",
    "On prend comme exemple 4 bins: ${[-1, -0.5]}$ ${[-0.5, 0]}$ ${[0, 0.5]}$ ${[0.5, 1]}$.\n",
    "\n",
    "**Plusieurs manières de construire un histogramme**: compter le nombre de valeurs, compter puis normaliser..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class DRMMinputGenerator:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, query_term_maxlen, intervals, normalize=False):\n",
    "        self.q_max_len = query_term_maxlen\n",
    "        self.intervals = intervals\n",
    "        self.normalize = normalize\n",
    "        self.intvlsArray = np.linspace(-1, 1, self.intervals)\n",
    "        \n",
    "    def set_params(self, model_wv, vectorizer):\n",
    "        self.model_wv = model_wv\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "    def hist(self, query, document):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for i in query.nonzero()[1]:\n",
    "            histo = []\n",
    "            for j in document.nonzero()[1]:\n",
    "                histo.append(cosine_similarity([self.model_wv.vectors[i]], [self.model_wv.vectors[j]])[0][0])\n",
    "            histo, bin_edges = np.histogram(histo, bins=self.intvlsArray)\n",
    "            if self.normalize:\n",
    "                histo = histo / histo.sum()\n",
    "            X.append(histo)\n",
    "        #retourner histogramme interactions + embeddings termes query\n",
    "        return np.array(X), np.array([self.model_wv.vectors[i] for i in query.nonzero()[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture du modèle\n",
    "\n",
    "D'après <a href=\"https://dl.acm.org/citation.cfm?id=2983769\">l'article</a>\n",
    "\n",
    "Code d'après <a href=\"https://github.com/X-Wei/ETH-thesis-TREC/blob/master/2-DRMM/DRMM.py\">ce génie</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Merge' from 'keras.layers' (/home/ismael/anaconda3/lib/python3.7/site-packages/keras/layers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-dc00592079e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from settings import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Merge' from 'keras.layers' (/home/ismael/anaconda3/lib/python3.7/site-packages/keras/layers/__init__.py)"
     ]
    }
   ],
   "source": [
    "N_HISTBINS = 30\n",
    "query_term_maxlen = 4\n",
    "embedding_size = 300\n",
    "\n",
    "#from settings import * \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, InputLayer, Flatten, Input, merge, Reshape\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "# helper function: model visualization \n",
    "def viz_model(model):\n",
    "    return SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "#### component 1: feed-forward network \n",
    "feed_forward = Sequential(\n",
    "    [Dense(input_dim = N_HISTBINS, output_dim=5, activation='tanh'),\n",
    "     Dense(output_dim=1, activation='tanh'),\n",
    "     ], # 30-5-1 as described in the paper\n",
    "    name='feed_forward_nw')\n",
    "\n",
    "\n",
    "#### component 2: gating network\n",
    "from keras.engine.topology import Layer\n",
    "class ScaledLayer(Layer): # a scaled layer\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaledLayer\n",
    "    , self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1] \n",
    "        self.W = self.add_weight(shape=(1,), # Create a trainable weight variable for this layer.\n",
    "                                 initializer='one', trainable=True)\n",
    "        super(ScaledLayer\n",
    "    , self).build(input_shape)  # Be sure to call this somewhere!\n",
    "    def call(self, x, mask=None):\n",
    "        return tf.mul(x, self.W)\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "input_idf = Input(shape=(None,), name='input_idf')\n",
    "scaled = ScaledLayer()(input_idf)\n",
    "gs = Activation('softmax', name='softmax')(scaled)\n",
    "gating = Model(input=input_idf, output=gs, name='gating')\n",
    "\n",
    "#### some helper functions to build the DRMM \n",
    "from keras.layers.core import Lambda\n",
    "# take the ith slice of the input x\n",
    "def slicei(x, i): return x[:,i,:]\n",
    "def slicei_output_shape(input_shape): return (input_shape[0], input_shape[2])\n",
    "\n",
    "# concatenate inputs into one tensor \n",
    "def concat(x): return K.concatenate(x) \n",
    "def concat_output_shape(input_shape): return (input_shape[0][0], input_shape[0][1])\n",
    "\n",
    "# innerproduct of zs and gs\n",
    "def innerprod(x): return K.sum( tf.mul(x[0],x[1]), axis=1)\n",
    "def innerprod_output_shape(input_shape): return (input_shape[0][0],1)\n",
    "\n",
    "# get the difference of input \n",
    "def diff(x): return tf.sub(x[0], x[1]) \n",
    "def diff_output_shape(input_shape): return input_shape[0]\n",
    "\n",
    "# custom loss function: hinge of (score_pos - score_neg)\n",
    "def pairwise_hinge(y_true, y_pred): # y_pred = score_pos - score_neg, **y_true doesn't matter here**\n",
    "    return K.mean( K.maximum(0.1 - y_pred, y_true*0.0) )  \n",
    "\n",
    "# self-defined metrics\n",
    "def ranking_acc(y_true, y_pred):\n",
    "    y_pred = y_pred > 0 \n",
    "    return K.mean(y_pred)\n",
    "\n",
    "def gen_DRMM_model(QLEN, feed_forward = feed_forward): \n",
    "    '''\n",
    "    generates a DRMM model for query length = `QLEN`\n",
    "    returns 2 items: `scoring_model, ranking_model`\n",
    "    * `scoring_model` is for predicting;\n",
    "    * `ranking_model` is for training, and ranking_model has another field: `initial_weights`, which is used in `KFold`\n",
    "    '''\n",
    "    # input 1: idf \n",
    "    input_idf = Input(shape=(QLEN,), name='input_idf')\n",
    "\n",
    "    # `gs` = gating output \n",
    "    # scaled = ScaledLayer(input_idf)\n",
    "    # gs = Activation('softmax', name='softmax')(scaled)\n",
    "    # gating = Model(input=input_idf, output=gs, name='gating')\n",
    "    gs = gating(input_idf)\n",
    "\n",
    "    # input 2: hist vectors, shape = QLEN x  N_HISTBINS\n",
    "    input_hists = Input(shape=(QLEN, N_HISTBINS), name='input_hists')\n",
    "    \n",
    "    # `zs` = feed_forward network output  \n",
    "    zs = [ feed_forward( Lambda(lambda x:slicei(x,i), slicei_output_shape, name='slice%d'%i)(input_hists) )\\\n",
    "            for i in xrange(QLEN) ] \n",
    "    zs = Lambda(concat, concat_output_shape, name='concat_zs')(zs)\n",
    "\n",
    "    # score = inner product of zs and gs \n",
    "    scores = Lambda(innerprod, innerprod_output_shape, name='innerprod_zs_gs')([zs, gs])\n",
    "\n",
    "    # scoring model \n",
    "    scoring_model = Model(input=[input_idf, input_hists], output=[scores], name='scoring_model')\n",
    "\n",
    "    # input3 -- the negative hists vector \n",
    "    input_hists_neg = Input(shape=(QLEN, N_HISTBINS), name='input_hists_neg')\n",
    "    zs_neg = [ feed_forward( Lambda(lambda x:slicei(x,i), slicei_output_shape, name='slice%d_neg'%i)(input_hists_neg) )\\\n",
    "          for i in xrange(QLEN) ]\n",
    "    zs_neg = Lambda(concat, concat_output_shape, name='concat_zs_neg')(zs_neg)\n",
    "    scores_neg = Lambda(innerprod, innerprod_output_shape, name='innerprod_zs_gs_neg')([zs_neg, gs])\n",
    "\n",
    "    ''' **NOTE** \n",
    "    for negative document's score, can't just do (don't know why...): \n",
    "        `scores_neg = scoring_model([input_idf, input_hists_neg])`\n",
    "    if we do like above, both outputs from `two_score_model` will be 0.0 \n",
    "    '''\n",
    "\n",
    "    two_score_model = Model(input=[input_idf, input_hists, input_hists_neg], \n",
    "                            output=[scores, scores_neg], name='two_score_model')\n",
    "    # positive score - negative score \n",
    "    posneg_score_diff = Lambda(diff, diff_output_shape, name='posneg_score_diff')([scores, scores_neg])\n",
    "    ranking_model = Model(input=[input_idf, input_hists, input_hists_neg], \n",
    "                            output=[posneg_score_diff], name='ranking_model')\n",
    "    \n",
    "    ranking_model.compile(optimizer='adagrad', loss=pairwise_hinge, metrics=[ranking_acc])\n",
    "    ranking_model.initial_weights = ranking_model.get_weights()\n",
    "    return scoring_model, ranking_model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
