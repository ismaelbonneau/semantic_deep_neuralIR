{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRMM (Deep Relevance Matching Model)\n",
    "\n",
    "_Ismaël Bonneau_\n",
    "\n",
    "Mots-clés: _Relevance Matching_, _Semantic Matching_, _Neural Models_,\n",
    "_Ad-hoc Retrieval_, _Ranking Models_\n",
    "\n",
    "\n",
    "#### But de ce notebook: Comprendre et construire une architecture **DRMM** fonctionnelle avec **pytorch**, et l'expliquer de façon concise.\n",
    "\n",
    "Un gros bisou à Daniel Godoy pour son tutoriel <a href=\"https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\">comprendre pytorch par l'exemple pas à pas</a> (en Anglais).\n",
    "\n",
    "Pour cela, 2 étapes:\n",
    "\n",
    "- construire la chaîne de pré traitements:\n",
    "    - générer des paires document-requête non pertinentes et pertinentes pour l'apprentissage\n",
    "    - générer des histogrammes d'interaction locales au niveau document-requête\n",
    "- construire l'architecture DRMM\n",
    "\n",
    "Les interractions sont pour le moment des interactions locales sur des word embeddings et sont mesurées comme une similarité cosinus entre les vecteurs des mots de la requête et ceux du document.\n",
    "\n",
    "## En quoi consiste le modèle DRMM?\n",
    "\n",
    "lien vers <a href=\"https://arxiv.org/pdf/1711.08611.pdf\">l'article</a> (en Anglais) par _Jiafeng Guo_, _Yixing Fan_, _Qingyao Ai_, _W. Bruce Croft_ (2017) [1]\n",
    "\n",
    "DRMM (Deep Relevance Matching Model) est un modèle de réseau de neurones profond pour la RI (recherche d'information).\n",
    "\n",
    "Un des objectifs principaux de la RI est de déterminer la **pertinence** d'un document (cela peut être un document court sous forme d'un paragraphe, ou long, voire très long) par rapport à une requête donnée. Un moteur de RI traditionnel retournera alors une liste ordonnée des documents par pertinence par rapport à une requête posée par l'utilisateur, avec en tête de liste les documents les plus pertinents.\n",
    "\n",
    "Un problème qui se pose avec de nombreux modèles de RI est . Certains termes de la requête ne se trouvent pas toujours dans des documents qui sont pourtant pertinents pour cette requête. Pensons à une requête sur la pigeons dans la ville de Paris: Un document qui aurait pour sujet le \"problème envahissant des oiseaux dans la capitale Française\", sans contenir une seule fois les mots Pigeons et Paris, aurait peu de chance d'être considéré comme pertinent.\n",
    "\n",
    "Plusieurs solutions sont donc proposées pour résoudre ce problème.\n",
    "\n",
    "DRMM est un modèle **orienté interactions**, qui applique une fonction apprise (un réseau de neurones n'est rien d'autres qu'une fonction très complexe) à des interactions entre un document et une requête, qui ne font donc pas partie du réseau et ne sont pas apprises. Cette fonction a pour but de calculer un **score pour la paire document-requête**, score d'autant **plus élevé que le document est pertinent pour la requête**.\n",
    "\n",
    "Il se compose de deux parties: \n",
    "\n",
    "- Une partie **\"feed forward matching network\"** qui est un simple perceptron multicouche. Il s'agit dans l'implémentation de [1] d'un perceptron à 3 couches, de taille 30 neurones, 5 neurones, et 1 neurone. Cette partie vient calculer un score pour l'interaction de chaque terme de la requête avec l'ensemble des termes du document. Pour une requête de 5 termes, le partie feed forward matching produira donc un vecteur de dimension 5, pour 1 score par terme de la requête. On a donc un bloc qui prend en entrée une matrice d'interactions des termes de la requête, et qui rend en sortie un vecteur de score pour chaque terme de la requête.\n",
    "\n",
    "- Une partie **\"term gating network\"** qui est un perceptron à une couche. Il s'agit uniquement d'apprendre un vecteur qui vient mutiplier chaque \"vecteur de terme\" de la requête et ensuitepondérer les scores calculés par la partie feed forward. \n",
    "\n",
    "<img src=\"images/DRMMschema.png\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "On utilise pour cet exemple le dataset <a href=\"https://trec.nist.gov/data/robust/04.guidelines.html\">robust 2004</a>. Il s'agit d'une collection de documents datés de 2004 provenant de journaux américains, le LA times, le Financial times, le Federal Register 94, et le Foreign Broadcast Information Service.\n",
    "\n",
    "\n",
    "| nombre de requêtes      |  jugements de pertinence    |\n",
    "| -------------:|  ------------- |\n",
    "|     **250**       |   **311,410**\n",
    "\n",
    "| collection      |     nombre de documents    |     taille en Mo    |\n",
    "| ------------- |: -------------: |: -------------: |\n",
    "| Financial times      |        210 158        |        564        |\n",
    "| La times       |        131,896        |        475        |\n",
    "| FBIS      |        130,471        |        470        |\n",
    "| FR94      |        55,630        |        395        |\n",
    "| **TOTAL**      |        **528,155**        |        **1904**        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import sep\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "embeddings_path = \"embeddings\"\n",
    "dataset_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré traitements: \n",
    "\n",
    "### Obtenir des word embeddings \n",
    "\n",
    "Ce word embedding a les caractéristiques suivantes:\n",
    "\n",
    "- Gensim word2vec Continuous Skipgram\n",
    "- taille de vecteur ${300}$\n",
    "- window ${10}$\n",
    "- entrainé sur la collection robust 2004\n",
    "- lemmatisation avec Krovetz, mots en minuscule\n",
    "- ${116832}$ mots\n",
    "\n",
    "voir script <a href=\"https://github.com/ismaelbonneau/semantic_deep_neuralIR/blob/master/scripts/generate_embeddings.py\">generate_embeddings.py</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word_vectors = gensim.models.Word2Vec.load('embeddings/model_1')\n",
    "word_vectors.init_sims(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('albi', 0.6107227802276611),\n",
       " ('montpellier', 0.5868463516235352),\n",
       " ('grenoble', 0.537689745426178),\n",
       " ('marseille', 0.5351033210754395),\n",
       " ('tarbe', 0.5294458866119385)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.wv.most_similar(\"toulouse\")[:5] # c'est nooooooormal la mif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Récupère les paires de pertinence/non pertinence pour chaque requête \n",
    "\n",
    "On génère un dictionnaire qui contient pour chaque requête en clé, un dictionnaire contenant 2 listes:\n",
    "- \"relevant\" contenant des id de document pertinents pour la requête.\n",
    "- \"irrelevant\" contenant des id de document non pertinents pour la requête au sens de qrels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On récupère les requêtes:\n",
    "\n",
    "Elles se trouvent sous forme de tuple ([mots clés], [texte de la requête]). On ne garde que les mots clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation\n",
    "from krovetzstemmer import Stemmer #stemmer pas mal pour la PR\n",
    "ks = Stemmer()\n",
    "\n",
    "def clean(txt):\n",
    "    return \" \".join([ks.stem(t) for t in txt.replace(\",\", \"\").replace(\".\", \"\").split(\" \")])   \n",
    "with open(dataset_path + sep + \"robust2004.txt\", \"r\") as f:\n",
    "    queries = ast.literal_eval(f.read())\n",
    "queries = {d:clean(queries[d][0]) for d in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new hydroelectric project\n",
      "ireland peace talks\n"
     ]
    }
   ],
   "source": [
    "print(queries[\"307\"])\n",
    "print(queries[\"404\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le DRMM a deux entrées: une entrée interactions et une entrée termes.\n",
    "\n",
    "L'entrée termes prend un vecteur d'idf des termes de la requête. Il faut donc pouvoir récupérer efficacement des idf. Pour cela, on construit un dictionnaire terme -> idf qui nous servira dans l'étape d'après.\n",
    "\n",
    "voir script <a href=\"https://github.com/ismaelbonneau/semantic_deep_neuralIR/blob/master/scripts/get_idf.py\">get_idf.py</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf marseille:  8.366004087237275\n"
     ]
    }
   ],
   "source": [
    "#bon là on charge du coup vu que le fichier est sauvegardé sur le disque\n",
    "idf = pickle.load(open(\"idf_robust2004.pkl\", \"rb\"))\n",
    "print(\"idf marseille: \", idf[\"marseille\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On prépare dans des fichiers les matrices d'interactions et les vecteurs d'idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query chargé\n",
      "relevance chargé\n",
      "docs chargés\n"
     ]
    }
   ],
   "source": [
    "from datasets import Robust04 #permet de gérer le chargement et le traitement de robust2004\n",
    "\n",
    "inputgenerator = Robust04(intervals=30, model_wv=word_vectors)\n",
    "inputgenerator.load_idf(idf_file=\"idf_robust2004.pkl\")\n",
    "inputgenerator.load_all_query(file_query=\"data/robust2004.txt\")\n",
    "inputgenerator.load_relevance(file_rel=\"data/qrels.robust2004.txt\")\n",
    "inputgenerator.load_all_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette méthode calcule les matrices d'interraction et les charge dans des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de requetes: 249.\n",
      "requete 301 complete.\n",
      "requete 302 complete.\n",
      "requete 303 complete.\n",
      "requete 304 complete.\n",
      "requete 305 complete.\n",
      "requete 306 complete.\n",
      "requete 307 complete.\n",
      "requete 308 complete.\n",
      "requete 309 complete.\n",
      "requete 310 complete.\n",
      "requete 311 complete.\n",
      "requete 312 complete.\n",
      "requete 313 complete.\n",
      "requete 314 complete.\n",
      "requete 315 complete.\n",
      "requete 316 complete.\n",
      "requete 317 complete.\n",
      "requete 318 complete.\n",
      "requete 319 complete.\n",
      "requete 320 complete.\n",
      "requete 321 complete.\n",
      "requete 322 complete.\n",
      "requete 323 complete.\n",
      "requete 324 complete.\n",
      "requete 325 complete.\n",
      "requete 326 complete.\n",
      "requete 327 complete.\n",
      "requete 328 complete.\n",
      "requete 329 complete.\n",
      "requete 330 complete.\n",
      "requete 331 complete.\n",
      "requete 332 complete.\n",
      "requete 333 complete.\n",
      "requete 334 complete.\n",
      "requete 335 complete.\n",
      "requete 336 complete.\n",
      "requete 337 complete.\n",
      "requete 338 complete.\n",
      "requete 339 complete.\n",
      "requete 340 complete.\n",
      "requete 341 complete.\n",
      "requete 342 complete.\n",
      "requete 343 complete.\n",
      "requete 344 complete.\n",
      "requete 345 complete.\n",
      "requete 346 complete.\n",
      "requete 347 complete.\n",
      "requete 348 complete.\n",
      "requete 349 complete.\n",
      "requete 350 complete.\n",
      "requete 351 complete.\n",
      "requete 352 complete.\n",
      "requete 353 complete.\n",
      "requete 354 complete.\n",
      "requete 355 complete.\n",
      "requete 356 complete.\n",
      "requete 357 complete.\n",
      "requete 358 complete.\n",
      "requete 359 complete.\n",
      "requete 360 complete.\n",
      "requete 361 complete.\n",
      "requete 362 complete.\n",
      "requete 363 complete.\n",
      "requete 364 complete.\n",
      "requete 365 complete.\n",
      "requete 366 complete.\n",
      "requete 367 complete.\n",
      "requete 368 complete.\n",
      "requete 369 complete.\n",
      "requete 370 complete.\n",
      "requete 371 complete.\n",
      "requete 372 complete.\n",
      "requete 373 complete.\n",
      "requete 374 complete.\n",
      "requete 375 complete.\n",
      "requete 376 complete.\n",
      "requete 377 complete.\n",
      "requete 378 complete.\n",
      "requete 379 complete.\n",
      "requete 380 complete.\n",
      "requete 381 complete.\n",
      "requete 382 complete.\n",
      "requete 383 complete.\n",
      "requete 384 complete.\n",
      "requete 385 complete.\n",
      "requete 386 complete.\n",
      "requete 387 complete.\n",
      "requete 388 complete.\n",
      "requete 389 complete.\n",
      "requete 390 complete.\n",
      "requete 391 complete.\n",
      "requete 392 complete.\n",
      "requete 393 complete.\n",
      "requete 394 complete.\n",
      "requete 395 complete.\n",
      "requete 396 complete.\n",
      "requete 397 complete.\n",
      "requete 398 complete.\n",
      "requete 399 complete.\n",
      "requete 400 complete.\n",
      "requete 401 complete.\n",
      "requete 402 complete.\n",
      "requete 403 complete.\n",
      "requete 404 complete.\n",
      "requete 405 complete.\n",
      "requete 406 complete.\n",
      "requete 407 complete.\n",
      "requete 408 complete.\n",
      "requete 409 complete.\n",
      "requete 410 complete.\n",
      "requete 411 complete.\n",
      "requete 412 complete.\n",
      "requete 413 complete.\n",
      "requete 414 complete.\n",
      "requete 415 complete.\n",
      "requete 416 complete.\n",
      "requete 417 complete.\n",
      "requete 418 complete.\n",
      "requete 419 complete.\n",
      "requete 420 complete.\n",
      "requete 421 complete.\n",
      "requete 422 complete.\n",
      "requete 423 complete.\n",
      "requete 424 complete.\n",
      "requete 425 complete.\n",
      "requete 426 complete.\n",
      "requete 427 complete.\n",
      "requete 428 complete.\n",
      "requete 429 complete.\n",
      "requete 430 complete.\n",
      "requete 431 complete.\n",
      "requete 432 complete.\n",
      "requete 433 complete.\n",
      "requete 434 complete.\n",
      "requete 435 complete.\n",
      "requete 436 complete.\n",
      "requete 437 complete.\n",
      "requete 438 complete.\n",
      "requete 439 complete.\n",
      "requete 440 complete.\n",
      "requete 441 complete.\n",
      "requete 442 complete.\n",
      "requete 443 complete.\n",
      "requete 444 complete.\n",
      "requete 445 complete.\n",
      "requete 446 complete.\n",
      "requete 447 complete.\n",
      "requete 448 complete.\n",
      "requete 449 complete.\n",
      "requete 450 complete.\n",
      "requete 601 complete.\n",
      "requete 602 complete.\n",
      "requete 603 complete.\n",
      "requete 604 complete.\n",
      "requete 605 complete.\n",
      "requete 606 complete.\n",
      "requete 607 complete.\n",
      "requete 608 complete.\n",
      "requete 609 complete.\n",
      "requete 610 complete.\n",
      "requete 611 complete.\n",
      "requete 612 complete.\n",
      "requete 613 complete.\n",
      "requete 614 complete.\n",
      "requete 615 complete.\n",
      "requete 616 complete.\n",
      "requete 617 complete.\n",
      "requete 618 complete.\n",
      "requete 619 complete.\n",
      "requete 620 complete.\n",
      "requete 621 complete.\n",
      "requete 622 complete.\n",
      "requete 623 complete.\n",
      "requete 624 complete.\n",
      "requete 625 complete.\n",
      "requete 626 complete.\n",
      "requete 627 complete.\n",
      "requete 628 complete.\n",
      "requete 629 complete.\n",
      "requete 630 complete.\n",
      "requete 631 complete.\n",
      "requete 632 complete.\n",
      "requete 633 complete.\n",
      "requete 635 complete.\n",
      "requete 636 complete.\n",
      "requete 637 complete.\n",
      "requete 638 complete.\n",
      "requete 639 complete.\n",
      "requete 640 complete.\n",
      "requete 641 complete.\n",
      "requete 642 complete.\n",
      "requete 643 complete.\n",
      "requete 644 complete.\n",
      "requete 645 complete.\n",
      "requete 646 complete.\n",
      "requete 647 complete.\n",
      "requete 648 complete.\n",
      "requete 649 complete.\n",
      "requete 650 complete.\n",
      "requete 651 complete.\n",
      "requete 652 complete.\n",
      "requete 653 complete.\n",
      "requete 654 complete.\n",
      "requete 655 complete.\n",
      "requete 656 complete.\n",
      "requete 657 complete.\n",
      "requete 658 complete.\n",
      "requete 659 complete.\n",
      "requete 660 complete.\n",
      "requete 661 complete.\n",
      "requete 662 complete.\n",
      "requete 663 complete.\n",
      "requete 664 complete.\n",
      "requete 665 complete.\n",
      "requete 666 complete.\n",
      "requete 667 complete.\n",
      "requete 668 complete.\n",
      "requete 669 complete.\n",
      "requete 670 complete.\n",
      "requete 671 complete.\n",
      "requete 673 complete.\n",
      "requete 674 complete.\n",
      "requete 675 complete.\n",
      "requete 676 complete.\n",
      "requete 677 complete.\n",
      "requete 678 complete.\n",
      "requete 679 complete.\n",
      "requete 680 complete.\n",
      "requete 681 complete.\n",
      "requete 682 complete.\n",
      "requete 683 complete.\n",
      "requete 684 complete.\n",
      "requete 685 complete.\n",
      "requete 686 complete.\n",
      "requete 687 complete.\n",
      "requete 688 complete.\n",
      "requete 689 complete.\n",
      "requete 690 complete.\n",
      "requete 691 complete.\n",
      "requete 692 complete.\n",
      "requete 693 complete.\n",
      "requete 694 complete.\n",
      "requete 695 complete.\n",
      "requete 696 complete.\n",
      "requete 697 complete.\n",
      "requete 698 complete.\n",
      "requete 699 complete.\n",
      "requete 700 complete.\n",
      "data completed\n"
     ]
    }
   ],
   "source": [
    "inputgenerator.prepare_data_forNN(\"saved_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette méthode pré calcule les matrices d'interraction des résultats de BM25 et les charge dans des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de requetes: 249.\n",
      "data completed\n"
     ]
    }
   ],
   "source": [
    "inputgenerator.prepare_data_reranking(pickle.load(open(\"results_bm25_robust.pkl\", \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On peut maintenant récupérer les matrices d'interraction pour les utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 requetes en train, 0 en test.\n"
     ]
    }
   ],
   "source": [
    "test_size = 0 # 20% de requêtes en test\n",
    "\n",
    "#random.seed(1401)\n",
    "\n",
    "query_idf = pickle.load(open(\"saved_data/query_idf.pkl\", \"rb\"))\n",
    "\n",
    "interractions_train_pos = []\n",
    "interractions_train_neg = []\n",
    "interractions_test_pos = []\n",
    "interractions_test_neg = []\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for id_requete in paires:\n",
    "    if id_requete != '634':\n",
    "        saintjeanlapuenta = np.load(\"saved_data/{}_interractions.npy\".format(id_requete))\n",
    "        if random.random() < test_size:\n",
    "            test.append(id_requete)\n",
    "            for neg, pos in zip(saintjeanlapuenta[::2], saintjeanlapuenta[1::2]):\n",
    "                interractions_test_pos.append([torch.from_numpy(pos).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "                interractions_test_neg.append([torch.from_numpy(neg).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "        else:\n",
    "            train.append(id_requete)\n",
    "            for neg, pos in zip(saintjeanlapuenta[::2], saintjeanlapuenta[1::2]):\n",
    "                interractions_train_pos.append([torch.from_numpy(pos).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "                interractions_train_neg.append([torch.from_numpy(neg).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "\n",
    "print(\"{} requetes en train, {} en test.\".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture du modèle\n",
    "\n",
    "### Avec pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MarginRankingLoss\n",
    "\n",
    "class AttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.Tensor(1, n))  # define the trainable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assuming x is of size b-1-h-w\n",
    "        return x * self.weights  # element-wise multiplication\n",
    "\n",
    "\n",
    "#hérite de la classe Pytorch Module\n",
    "class DRMM(torch.nn.Module):\n",
    "    def __init__(self, hist_size, query_term_maxlen, use_cuda=True):\n",
    "        super(DRMM, self).__init__()\n",
    "        self.MLP0 = torch.nn.Linear(hist_size, 5)\n",
    "        self.MLP1 = torch.nn.Linear(5, 1)\n",
    "        torch.nn.init.xavier_normal_(self.MLP0.weight, gain = torch.nn.init.calculate_gain('tanh'))\n",
    "        torch.nn.init.xavier_normal_(self.MLP1.weight, gain = torch.nn.init.calculate_gain('tanh'))\n",
    "        #initialisation du vecteur de term gating\n",
    "        \n",
    "        self.termgating = AttentionLayer(query_term_maxlen)\n",
    "        torch.nn.init.xavier_normal_(self.termgating.weights, gain = torch.nn.init.calculate_gain('linear'))\n",
    "        \n",
    "    #méthode forward à redéfinir\n",
    "    def forward(self, interractions, termvector):\n",
    "        \"\"\"\n",
    "        interractions: (query_term_maxlen, hist_size)\n",
    "        termvector: \n",
    "        \"\"\"\n",
    "        #partie histogramme\n",
    "        interractions_output = self.MLP0(interractions)\n",
    "        interractions_output = torch.nn.functional.tanh(interractions_output)\n",
    "        interractions_output = self.MLP1(interractions_output)\n",
    "        interractions_output = torch.nn.functional.tanh(interractions_output).squeeze() #passe de (query_term_maxlen, 1) à (1, query_term_maxlen)\n",
    "        #partie term gating \n",
    "        gating_output = torch.nn.functional.softmax(self.termgating(termvector).squeeze())\n",
    "        #combiner les 2 avec un produit scalaire\n",
    "        axis = 0\n",
    "        #si on travaille par mini-batch:\n",
    "        if len(interractions.size()) == 3:\n",
    "            axis = 1\n",
    "        s = torch.sum(gating_output * interractions_output, dim = axis) #dim = 0 ne marche qui pas batch.\n",
    "        return s\n",
    "    \n",
    "    def get_model_size(self):\n",
    "        \"\"\"\n",
    "        retourne le nombre de paramètres du modèle.\n",
    "        \"\"\"\n",
    "        return sum([p.size(0) if len(p.size()) == 1 else p.size(0)*p.size(1) for p in self.parameters()])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"DRMM with {} parameters. ti as compris\".format(self.get_model_size())\n",
    "    \n",
    "    \n",
    "class HingeLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Hinge Loss\n",
    "          max(0, 1-x+y)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        output = 1-x+y\n",
    "        return output.clamp(min=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On va créer une classe de Dataset qui contient les paires pertinentes et les paires non pertinentes.\n",
    "\n",
    "Plus précisément, la classe Dataset de Pytorch est un wrapper pour X (traditionnellement les vecteurs de features) et Y (traditionnellement les labels). Ici, on a pas besoin de vecteur de labels. On peut donc transformer l'utilisation de la classe pour mettre dans X une matrice de paires (interractions relevant, vecteur de termes relevant) et dans Y la même matrice, pour les paires non pertinentes. L'avantage est que lors du shuffle avec le DataLoader, on conservera l'alignement doc pertinent/non pertinent pour une requête et on ne sera pas embêté pour utiliser la ranking hinge loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DrmmDataset(Dataset):\n",
    "    def __init__(self, pos_tensor, neg_tensor):\n",
    "        self.x = pos_tensor\n",
    "        self.y = neg_tensor\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'étape de création du DataLoader:\n",
    "\n",
    "Cette classe va nous permettre de gérer le découpage en mini batches et le shuffle. C'est un wrapper pour la classe DataSet qui fournit entre autres un itérateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DrmmDataset(interractions_train_pos, \n",
    "                            interractions_train_neg)\n",
    "val_dataset = DrmmDataset(interractions_test_pos,\n",
    "                          interractions_test_neg)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batchsize = 20\n",
    "#classe utile pour gérer les mini batches et le shuffle (crucial!)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batchsize, shuffle=True)\n",
    "#shuffle = False pour le DataLoader de test!\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On regroupe toutes les étapes de forward/backprop sur un mini-batch dans une fonction:\n",
    "\n",
    "La fonction prend deux mini batch en entrée et:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drmm_make_train_step(model, loss_fn, optimizer):\n",
    "    def drmm_train_step(pos_batch, neg_batch):\n",
    "        #mettre le modèle en mode train\n",
    "        model.train()\n",
    "        #étape forward...\n",
    "        pos_score = model(pos_batch[0], pos_batch[1])\n",
    "        neg_score = model(neg_batch[0], neg_batch[1])\n",
    "        #calcul de la loss\n",
    "        loss = loss_fn(pos_score, neg_score, torch.Tensor([1] * pos_batch[0].size()[0]))\n",
    "        #loss = loss_fn(pos_score, neg_score)\n",
    "        #calcul des gradients\n",
    "        loss.backward()\n",
    "        #mise à jour des paramètres\n",
    "        optimizer.step()\n",
    "        #reset des gradients après le passage sur ce batch\n",
    "        optimizer.zero_grad()\n",
    "        #retourner la loss\n",
    "        return loss.item() #.item()\n",
    "    \n",
    "    return drmm_train_step\n",
    "\n",
    "def drmm_make_val_step(model, loss_fn, optimizer):\n",
    "    def drmm_val_step(pos_batch, neg_batch):\n",
    "        #mettre le modèle en mode test\n",
    "        model.eval()\n",
    "        #étape forward...\n",
    "        pos_score = model(pos_batch[0], pos_batch[1])\n",
    "        neg_score = model(neg_batch[0], neg_batch[1])\n",
    "        \n",
    "        #loss = loss_fn(pos_score, neg_score, torch.Tensor([1] * pos_batch[0].size()[0]))\n",
    "        loss = loss_fn(pos_score, neg_score)\n",
    "        \n",
    "        return loss.item()\n",
    "    return drmm_val_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Et... l'apprentissage du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRMM with 162 parameters. ti as compris\n"
     ]
    }
   ],
   "source": [
    "drmm = DRMM(30, 4)\n",
    "print(drmm)\n",
    "\n",
    "hingeloss = MarginRankingLoss(margin=1.)\n",
    "optimizer = torch.optim.Adam(drmm.parameters(), lr=0.0005)\n",
    "\n",
    "bm25_dict = pickle.load(open(\"results_bm25_robust.pkl\", \"rb\"))\n",
    "reranker = Reranker(bm25_dict)\n",
    "\n",
    "make_train_step = drmm_make_train_step(drmm, hingeloss, optimizer)\n",
    "make_val_step = drmm_make_val_step(drmm, hingeloss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "metrics = []\n",
    "val_metrics = []\n",
    "nb_epochs = 50\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    #parcourir tous les batches du dataloader\n",
    "    tiacompris = []\n",
    "    for pos_batch, neg_batch in train_loader:\n",
    "\n",
    "        #pos_batch[0]: les interractions positives\n",
    "        #pos_batch[1]: les query term\n",
    "        #neg_batch[0]: les interractions négatives correspondantes...\n",
    "        #neg_batch[1]: les query term correspondantes...\n",
    "        \n",
    "        #évaluation empirical loss + backward pass\n",
    "        tiacompris.append(make_train_step(pos_batch, neg_batch))\n",
    "    losses.append(np.array(tiacompris).mean())\n",
    "    \n",
    "    #évaluation RI\n",
    "    #reranker.set_model(drmm)\n",
    "    #lol = reranker.rerank(queries = train)\n",
    "    #reranker.save_results(lol, \"tiacompris\")\n",
    "    #treceval = compute_trec_eval(\"data/qrels.robust2004.txt\", \"tiacompris\")\n",
    "    #metrics.append((treceval[\"map\"], treceval[\"P_20\"]))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"**epoch {}: loss {}**\".format(i, losses[-1]))\n",
    "        #print(\"**epoch {}: loss {} map {} P_20 {}**\".format(i, losses[-1], treceval[\"map\"], treceval[\"P_20\"]))\n",
    "    \n",
    "    #on met le modèle en mode \"ne calcule pas le gradient\"\n",
    "    #with torch.no_grad():\n",
    "        #on parcourt\n",
    "        #for pos_batch_val, neg_batch_val in val_loader:    \n",
    "            #val_loss = make_val_step(pos_batch_val, neg_batch_val)\n",
    "        #val_losses.append(val_loss)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(list(range(nb_epochs)), losses, label=\"train\")\n",
    "#plt.plot(list(range(nb_epochs)), val_losses, label=\"validation\")\n",
    "plt.title(\"DRRM loss sur robust-2004\", fontsize=16)\n",
    "plt.ylabel(\"loss\", fontsize=13)\n",
    "plt.xlabel(\"epochs\", fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut maintenant tester le modèle avec des métriques de RI:\n",
    "\n",
    "Pour chacune des requêtes de test, on récupère les 2000 premiers documents retournés par *BM25*. Nous procédons en réordonnant cette liste de documents avec notre modèle *DRMM* appris, et nous calculons des performances sur les 1000 premiers documents de ce re ranking sur un ensemble de métriques de RI (**NDCG@20**, **MAP**, **P@20**), en le comparant avec le ranking original *BM25*.\n",
    "\n",
    "Pour BM25, l'indexation a été faite comme suit:\n",
    "\n",
    "- indexation sur le texte du document et non le titre\n",
    "- texte sans stopwords, ponctuation, balises\n",
    "- stemming avec Krovetz\n",
    "\n",
    "<img src=\"images/learning_to_rank.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reranker:\n",
    "    def __init__(self, bm25_dict):\n",
    "        self.bm25_dict = bm25_dict #le dictionnaire query -> 2000 docs relevants pour bm25\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def rerank(self, queries=None):\n",
    "        \n",
    "        #queries: une liste d'ID de requete pour lesquels on veut réordonner les résultats\n",
    "        if queries == None:\n",
    "            queries_to_rerank = list(self.bm25_dict.keys())\n",
    "        else:\n",
    "            queries_to_rerank = queries\n",
    "        \n",
    "        #pour chaque requete, on va charger réordonner ses résultats avec le modèle\n",
    "        query_idf = pickle.load(open(\"saved_data/query_idf.pkl\", \"rb\"))\n",
    "        reranked_dict = {}\n",
    "        for id_requete in queries_to_rerank:\n",
    "            if id_requete != \"634\":\n",
    "                with torch.no_grad():\n",
    "                    #contient une matrice (2000, query_max_len, hist_size)\n",
    "                    saintjeanlapuenta = np.load(\"data/bm25_robust/{}_interractions.npy\".format(id_requete))\n",
    "                    a = np.tile(np.array([query_idf[id_requete]]), (saintjeanlapuenta.shape[0],1))\n",
    "\n",
    "                    model_scores = self.model(saintjeanlapuenta, a)\n",
    "                    lol = np.argsort(model_scores)[::-1] #tri décroissant\n",
    "\n",
    "                    # reranked: liste de tuples (document_id, score)\n",
    "                    reranked_dict[id_requete] = [(self.bm25_dict[id_requete][i][0], model_scores[i]) for i in lol]\n",
    "\n",
    "        return reranked_dict\n",
    "    \n",
    "    def get_results(self, id_requete, rank_list):\n",
    "        results = []\n",
    "        for i, (doc_id, score) in enumerate(rank_list[:1000]):\n",
    "            results.append(\" \".join([id_requete, \"Q0\", doc_id, str(i + 1), str(score), \"EARIA\"]))\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def save_results(self, rank_dict, res_file):\n",
    "        \"\"\"\n",
    "        sauver sur un fichier au format attendu par TREC\n",
    "        un dictionnaire query_id -> list (doc_id, score)\n",
    "        \"\"\"\n",
    "        results = [f\"{id_requete} Q0 EMPTY 1001 -100000 EARIA\" for id_requete in rank_dict]\n",
    "        for id_requete in rank_dict:\n",
    "            results.extend(self.get_results(id_requete, rank_dict[id_requete]))\n",
    "        \n",
    "        with open(res_file, \"w\") as tiacompris:\n",
    "            tiacompris.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def compute_trec_eval(qrel_file_path, resp_file_name):\n",
    "    command = [\"trec_eval/trec_eval\" , \"-c\", \"-M1001\", \"-m\", \"map\",\"-m\", \"P.20\", qrel_file_path, resp_file_name]\n",
    "    #Le fichier trec_eval est à récuperer ici https://github.com/usnistgov/trec_eval\n",
    "    # une fois clone, il faut faire un make dans le dossier et autoriser à l'execution (chmod +x)\n",
    "    completed_process = subprocess.run(command, capture_output=True)\n",
    "    results = completed_process.stdout.decode(\"utf-8\")\n",
    "    \n",
    "    with open(\"caca\", \"w\") as f:\n",
    "        f.write(results)\n",
    "    \n",
    "    total_score = {}\n",
    "    for tkt in results.split(\"\\n\")[:2]:\n",
    "        total_score[tkt.split(\"\\t\")[0].strip()] = float(tkt.split(\"\\t\")[2].strip())\n",
    "    \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(id_requete, rank_list):\n",
    "    results = []\n",
    "    for i, (doc_id, score) in enumerate(rank_list[:1000]):\n",
    "        results.append(\" \".join([id_requete, \"Q0\", doc_id, str(i + 1), str(score), \"EARIA\"]))\n",
    "    return results\n",
    "            \n",
    "    \n",
    "def save_results(rank_dict, res_file, queries):\n",
    "    \"\"\"\n",
    "    sauver sur un fichier au format attendu par TREC\n",
    "    un dictionnaire query_id -> list (doc_id, score)\n",
    "    \"\"\"\n",
    "    results = [f\"{id_requete} Q0 EMPTY 1001 -100000 EARIA\" for id_requete in queries]\n",
    "    for id_requete in queries:\n",
    "        results.extend(get_results(id_requete, rank_dict[id_requete]))\n",
    "        \n",
    "    with open(res_file, \"w\") as tiacompris:\n",
    "        tiacompris.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP et précision avec bm25 seul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': 0.2435, 'P_20': 0.3616}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_dict = pickle.load(open(\"results_bm25_robust.pkl\", \"rb\"))\n",
    "save_results(bm25_dict, \"proutpipi\", queries=train)\n",
    "compute_trec_eval(\"data/qrels.robust2004.txt\", \"proutpipi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP et précision sur le re ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map': 0.0159, 'P_20': 0.0183}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_dict = pickle.load(open(\"results_bm25_robust.pkl\", \"rb\"))\n",
    "reranker = Reranker(bm25_dict)\n",
    "reranker.set_model(drmm)\n",
    "lol = reranker.rerank(queries = train)\n",
    "reranker.save_results(lol, \"tiacompris\")\n",
    "compute_trec_eval(\"data/qrels.robust2004.txt\", \"tiacompris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perf en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map': 0.0011, 'P_20': 0.0022}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = reranker.rerank(queries = test)\n",
    "reranker.save_results(lol, \"stfe\")\n",
    "compute_trec_eval(\"data/qrels.robust2004.txt\", \"stfe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographie\n",
    "\n",
    "\n",
    "[1] <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.161&rep=rep1&type=pdf\">What Works Better for Question Answering: Stemming or\n",
    "Morphological Query Expansion?</a>\n",
    "\n",
    "[2] <a href=\"https://dl.acm.org/citation.cfm?id=160718\" >Viewing morphology as an inference process</a>\n",
    "\n",
    "[3] <a href=\"https://arxiv.org/pdf/1809.01682.pdf\">Deep Relevance Ranking Using Enhanced Document-Query Interactions</a>\n",
    "\n",
    "[4] <a href=\"http://delivery.acm.org/10.1145/2810000/2806475/p1411-kenter.pdf?ip=132.227.125.83&id=2806475&acc=ACTIVE%20SERVICE&key=7EBF6E77E86B478F%2EA72B4D473219EA0C%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1562588994_ca050bde6ad37b2c0d433db2e5df63ba\">Short Text Similarity with Word Embeddings</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
