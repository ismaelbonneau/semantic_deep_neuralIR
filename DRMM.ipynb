{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRMM (Deep Relevance Matching Model)\n",
    "\n",
    "_Ismaël Bonneau_\n",
    "\n",
    "\n",
    "But de ce notebook: Construire une architecture DRMM fonctionnelle avec **pytorch**.\n",
    "\n",
    "Pour cela, 2 étapes:\n",
    "\n",
    "- construire la chaîne de pré traitements:\n",
    "    - générer des paires document-requête non pertinentes et pertinentes pour l'apprentissage\n",
    "    - générer des histogrammes d'interaction locales au niveau document-requête\n",
    "- construire l'architecture DRMM\n",
    "\n",
    "Les interractions sont pour le moment des interactions locales sur des word embeddings et sont mesurées comme une similarité cosinus entre les vecteurs des mots de la requête et ceux du document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import sep\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "embeddings_path = \"embeddings_wiki2017\"\n",
    "dataset_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré traitements: \n",
    "\n",
    "### Récupérer des word embeddings \n",
    "\n",
    "Ce word embedding a les caractéristiques suivantes:\n",
    "\n",
    "- FastText Continuous Skipgram\n",
    "- taille de vecteur ${300}$\n",
    "- window ${5}$\n",
    "- entrainé sur wikipédia février 2017 en langue anglaise\n",
    "- pas de lemmatisation\n",
    "- ${302815}$ mots\n",
    "\n",
    "http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('embeddings_wiki2017/parameters.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Récupère les paires de pertinence/non pertinence pour chaque requête \n",
    "\n",
    "On génère un dictionnaire qui contient pour chaque requête en clé, un dictionnaire contenant 2 listes:\n",
    "- \"relevant\" contenant des id de document pertinents pour la requête.\n",
    "- \"irrelevant\" contenant des id de document non pertinents pour la requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paires = {}\n",
    "\n",
    "with open(dataset_path + sep + \"qrels.robust2004.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        lol = line.split()\n",
    "        paires.setdefault(lol[0], {})\n",
    "        paires[lol[0]].setdefault('relevant', []) \n",
    "        paires[lol[0]].setdefault('irrelevant', [])\n",
    "        if lol[-1] == '1':\n",
    "            paires[lol[0]][\"relevant\"].append(lol[2])\n",
    "        else:\n",
    "            paires[lol[0]][\"irrelevant\"].append(lol[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On récupère les requêtes:\n",
    "\n",
    "Elles se trouvent sous forme de tuple ([mots clés], [texte de la requête]). On ne garde que les mots clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation\n",
    "\n",
    "def clean(txt):\n",
    "    return txt.replace(\",\", \"\").replace(\".\", \"\")\n",
    "\n",
    "with open(dataset_path + sep + \"robust2004.txt\", \"r\") as f:\n",
    "    queries = ast.literal_eval(f.read())\n",
    "queries = {d:clean(queries[d][0]) for d in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international organized crime\n",
      "foreign minorities germany\n"
     ]
    }
   ],
   "source": [
    "print(queries[\"301\"])\n",
    "print(queries[\"401\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le DRMM a deux entrées: une entrée interactions et une entrée termes.\n",
    "\n",
    "L'entrée termes prend un vecteur d'idf des termes de la requête. Il faut donc pouvoir récupérer efficacement des idf. Pour cela, on construit un dictionnaire terme -> idf qui nous servira dans l'étape d'après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bon là on charge du coup vu que le fichier est sauvegardé sur le disque\n",
    "idf = pickle.load(open(\"idf_robust2004.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On prépare dans des fichiers les matrices d'interactions et les vecteurs d'idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query chargé\n",
      "relevance chargé\n"
     ]
    }
   ],
   "source": [
    "from load_data import Dataset #permet de gérer le chargement et le traitement de robust2004\n",
    "\n",
    "inputgenerator = Dataset(intervals=30, model_wv=model)\n",
    "inputgenerator.load_idf(idf_file=\"idf_robust2004.pkl\")\n",
    "inputgenerator.load_all_query(file_query=\"data/robust2004.txt\")\n",
    "inputgenerator.load_relevance(file_rel=\"data/qrels.robust2004.txt\")\n",
    "inputgenerator.load_all_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette méthode calcule les matrices d'interraction et les charge dans des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputgenerator.prepare_data_forNN(\"saved_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On peut maintenant récupérer les matrices d'interraction pour les utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "query_idf = pickle.load(open(\"saved_data/query_idf.pkl\", \"rb\"))\n",
    "\n",
    "interractions_train_pos = []\n",
    "interractions_train_neg = []\n",
    "interractions_test_pos = []\n",
    "interractions_test_neg = []\n",
    "\n",
    "for id_requete in paires:\n",
    "    if id_requete != '634':\n",
    "        saintjeanlapuenta = np.load(\"saved_data/{}_interractions.npy\".format(id_requete))\n",
    "        if random.random() < test_size:\n",
    "            for neg, pos in zip(saintjeanlapuenta[::2], saintjeanlapuenta[1::2]):\n",
    "                interractions_test_pos.append(np.array([pos, np.array([query_idf[id_requete]])]))\n",
    "                interractions_test_neg.append(np.array([neg, np.array([query_idf[id_requete]])]))\n",
    "        else:\n",
    "            for neg, pos in zip(saintjeanlapuenta[::2], saintjeanlapuenta[1::2]):\n",
    "                interractions_train_pos.append(np.array([pos, np.array([query_idf[id_requete]])]))\n",
    "                interractions_train_neg.append(np.array([neg, np.array([query_idf[id_requete]])]))\n",
    "\n",
    "\n",
    "interractions_train_pos = np.array(interractions_train_pos)\n",
    "interractions_train_neg = np.array(interractions_train_neg)\n",
    "interractions_test_pos = np.array(interractions_test_pos)\n",
    "interractions_test_neg = np.array(interractions_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13332, 2) (3049, 2)\n"
     ]
    }
   ],
   "source": [
    "print(interractions_train_pos.shape, interractions_test_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interractions_train_pos[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture du modèle\n",
    "\n",
    "### Avec pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRMM(torch.nn.Module):\n",
    "    def __init__(self, hist_size, query_term_maxlen, hidden_sizes=[128,64,16,1], use_cuda=True):\n",
    "        \n",
    "        if hidden_sizes[-1] != 1:\n",
    "            raise ValueError(\"la taille du dernier layer doit être 1\")\n",
    "            \n",
    "        super(DRMM, self).__init__()\n",
    "        self.MLP = [torch.nn.Linear(hist_size, hidden_sizes[0])] + [\n",
    "            torch.nn.Linear(hidden_sizes[i-1], hidden_sizes[i]) for i in range(1, len(hidden_sizes))]\n",
    "        for i in range(len(self.MLP)):\n",
    "            torch.nn.init.xavier_normal_(self.MLP[i].weight, gain = torch.nn.init.calculate_gain('tanh'))\n",
    "            \n",
    "        #initialisation du vecteur de term gating\n",
    "        self.termgating = torch.nn.Linear(query_term_maxlen, query_term_maxlen, bias=False)\n",
    "        torch.nn.init.xavier_normal_(self.termgating.weight, gain = torch.nn.init.calculate_gain('tanh'))\n",
    "        \n",
    "    def forward(self, interractions, termvector):\n",
    "        \n",
    "        #partie histogramme\n",
    "        interractions_output = self.MLP[0](interractions)\n",
    "        interractions_output = torch.nn.functional.tanh(interractions_output)\n",
    "        for layer in self.MLP[1:]:\n",
    "            interractions_output = layer(interractions_output)\n",
    "            interractions_output = torch.nn.functional.tanh(interractions_output)\n",
    "        interractions_output = interractions_output.squeeze()\n",
    "        \n",
    "        #partie term gating \n",
    "        gating_output = torch.nn.functional.softmax(self.termgating(termvector)).squeeze()\n",
    "        \n",
    "        #combiner les 2 avec un produit scalaire\n",
    "        s = torch.sum(interractions_output * gating_output, dim = 0)\n",
    "        \n",
    "        return s\n",
    "\n",
    "    \n",
    "class HingeLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Hinge Loss\n",
    "          max(0, 1-x+y)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        output = 1 - x + y\n",
    "        return output.clamp(min = 0).mean()\n",
    "    \n",
    "    \n",
    "def get_model_size(model):\n",
    "    return sum([ p.size(0) if len(p.size()) == 1 else p.size(0)*p.size(1) for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrmmDataset(Dataset):\n",
    "    def __init__(self, pos_tensor, neg_tensor):\n",
    "        self.x = pos_tensor\n",
    "        self.y = neg_tensor\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def tiacompris(self):\n",
    "        return \"ti as compris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DrmmDataset((), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('termgating.weight', tensor([[ 0.3789, -0.5197,  0.4738, -0.5709, -1.0463],\n",
      "        [ 0.8123, -0.8861,  1.2484, -0.6989,  0.6106],\n",
      "        [-1.0772,  0.8239,  0.8858, -0.4145, -0.3103],\n",
      "        [ 0.4447, -0.2343, -1.6786,  0.8777, -1.2764],\n",
      "        [-0.9384, -0.6642, -0.8697,  0.1279,  0.3763]]))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 26648 is out of bounds for axis 0 with size 26648",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-ff92555027eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscore_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterractions_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midf_vec_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mscore_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterractions_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midf_vec_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhingeloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 26648 is out of bounds for axis 0 with size 26648"
     ]
    }
   ],
   "source": [
    "drmm = DRMM(30,5)\n",
    "\n",
    "print(drmm.state_dict())\n",
    "\n",
    "hingeloss = HingeLoss()\n",
    "optimizer = torch.optim.Adam(drmm.parameters(), lr = 0.0005)\n",
    "\n",
    "nb_epochs = 10\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    #parcourir le dataloader\n",
    "\n",
    "    score_pos = drmm(prout(interractions_train[i]), prout(idf_vec_train[i]))\n",
    "    score_neg = drmm(prout(interractions_train[i+1]), prout(idf_vec_train[i+1]))\n",
    "\n",
    "    loss = hingeloss(score_pos, score_neg)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(drmm.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
