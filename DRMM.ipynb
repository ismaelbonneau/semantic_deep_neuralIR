{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRMM (Deep Relevance Matching Model)\n",
    "\n",
    "_Ismaël Bonneau_\n",
    "\n",
    "\n",
    "But de ce notebook: Construire une architecture DRMM fonctionnelle avec Keras.\n",
    "\n",
    "Pour cela, 2 étapes:\n",
    "\n",
    "- construire la chaîne de pré traitements:\n",
    "    - générer des paires document-requête non pertinentes et pertinentes pour l'apprentissage\n",
    "    - générer des histogrammes d'interaction locales au niveau document-requête\n",
    "- construire l'architecture DRMM\n",
    "\n",
    "Les interractions sont pour le moment des interactions locales sur des word embeddings et sont mesurées comme une similarité cosinus entre les vecteurs des mots de la requête et ceux du document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import sep\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "embeddings_path = \"embeddings_wiki2017\"\n",
    "dataset_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré traitements: \n",
    "\n",
    "### Récupérer des word embeddings \n",
    "\n",
    "Ce word embedding a les caractéristiques suivantes:\n",
    "\n",
    "- Gensim Continuous Skipgram\n",
    "- taille de vecteur ${300}$\n",
    "- window ${5}$\n",
    "- entrainé sur wikipédia février 2017 en langue anglaise\n",
    "- lemmatisation\n",
    "- ${273992}$ mots\n",
    "\n",
    "http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(embeddings_path + sep + \"model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [w for w in model.vocab]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word', vocabulary=vocabulary, binary=True, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Montpellier', 0.7763358354568481),\n",
       " ('Rennes', 0.7535181045532227),\n",
       " ('Nantes', 0.7404719591140747),\n",
       " ('Marseille', 0.7251037359237671),\n",
       " ('nîmes', 0.7154472470283508),\n",
       " ('Narbonne', 0.7153515815734863),\n",
       " ('Béziers', 0.7148354649543762),\n",
       " ('Poitiers', 0.7118483781814575),\n",
       " ('Perpignan', 0.7109518051147461),\n",
       " ('Bordeaux', 0.7101389765739441)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"Toulouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Récupère les paires de pertinence/non pertinence pour chaque requête \n",
    "\n",
    "On génère un dictionnaire qui contient pour chaque requête en clé, un dictionnaire contenant 2 listes:\n",
    "- \"relevant\" contenant des id de document pertinents pour la requête.\n",
    "- \"irrelevant\" contenant des id de document non pertinents pour la requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paires = {}\n",
    "\n",
    "with open(dataset_path + sep + \"qrels.robust2004.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        lol = line.split()\n",
    "        paires.setdefault(lol[0], {})\n",
    "        paires[lol[0]].setdefault('relevant', []) \n",
    "        paires[lol[0]].setdefault('irrelevant', [])\n",
    "        if lol[-1] == '1':\n",
    "            paires[lol[0]][\"relevant\"].append(lol[2])\n",
    "        else:\n",
    "            paires[lol[0]][\"irrelevant\"].append(lol[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On récupère les requêtes:\n",
    "\n",
    "Elles se trouvent sous forme de tuple ([mots clés], [texte de la requête]). On ne garde que les mots clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation\n",
    "\n",
    "def clean(txt):\n",
    "    return txt.replace(\",\", \"\").replace(\".\", \"\")\n",
    "\n",
    "with open(dataset_path + sep + \"robust2004.txt\", \"r\") as f:\n",
    "    queries = ast.literal_eval(f.read())\n",
    "queries = {d:clean(queries[d][0]) for d in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international organized crime\n",
      "foreign minorities germany\n"
     ]
    }
   ],
   "source": [
    "print(queries[\"301\"])\n",
    "print(queries[\"401\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_term_maxlen(queries):\n",
    "    return np.max([len(queries[q].split()) for q in queries])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le DRMM a deux entrées: une entrée interactions et une entrée termes.\n",
    "\n",
    "L'entrée termes prend un vecteur d'idf des termes de la requête. Il faut donc pouvoir récupérer efficacement des idf. Pour cela, on construit un dictionnaire terme -> idf qui nous servira dans l'étape d'après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bon là on charge du coup vu que le fichier est sauvegardé sur le disque\n",
    "idf = pickle.load(open(\"idf_robust2004.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le sud-est discriminé par rapport au sud-ouest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toulouse:  8.741735394187142 marseille:  9.294735943003845\n"
     ]
    }
   ],
   "source": [
    "print(\"toulouse: \", idf['toulouse'], \"marseille: \", idf['marseille'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ismael:  9.59792220199159 Laure:  10.904173855437945\n"
     ]
    }
   ],
   "source": [
    "print(\"Ismael: \", idf['ismael'], \"Laure: \", idf['laure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch as es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = es.Elasticsearch(['localhost:9200'], timeout=30, max_retries=10, retry_on_timeout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'robust-2004',\n",
       " '_type': '_doc',\n",
       " '_id': 'FBIS3-2172',\n",
       " '_version': 1,\n",
       " '_seq_no': 267959,\n",
       " '_primary_term': 1,\n",
       " 'found': True,\n",
       " '_source': {'text': 'FBIS drchi c FBIS CHI Document Type Daily Report Mar Central Eurasia Outlines Tasks Facing Government OW Beijing XINHUA in English GMT Mar OW Beijing XINHUA Language English Article Type BFN Text Moscow March XINHUA President Boris Yeltsin today underlined the main tasks facing the Russian Government stressing the urgent need for energetic reform of the country s economic mechanism The economic situation remains complicated this year and the social and political conditions are quite tough Yeltsin told an extended government meeting in the Kremlin He pointed out that the government already has the reform program that it adopted last August but it needs to be specified and implemented We need a policy oriented towards the future and it is wrong to call for reforms that are too costly and burdensome for people to tolerate Yeltsin said The president stressed that the main government action should be to forge an energetic transformation of the economic mechanism and it should shoulder the bulk of the problems as before He described the privatization program which started last year as the major item of the reform package and stressed that it must be completed as scheduled and without hindrances It is necessary therefore to create a legal framework and organizational mechanism to attract resources through investments by domestic and foreign businessmen for a technological renewal of production he said The president also said that priorities should be given to curbing inflation taxation payment and banking system reforms as well as agricultural reforms which included ensuring the rights of private landowners He went on The government is prepared to usher in radical reforms this year into the system of management of state owned enterprises or to be more exact create it a new from scratch The president s statement made clear that the demonopolization of the Russian economy will take time This is why preventive measures are particularly important to block the formation of new monopoly structures he explained Stressing the need for the effective mechanism of state supervision Yeltsin said that a draft civil code for Russia and other priority draft bills will be submitted to the State Duma lower house of parliament in the near future Their promulgation into law will lay a firm legal basis for the market economy and help Russian enterprises establish a firm footing the president said Commenting on the coal mine strikes Yeltsin said It is inadmissible to delay the payment of wages for several months I am closely watching the developments and the position of the labor unions',\n",
       "  'title': 'outlines tasks facing government'}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.get(index=\"robust-2004\", id=\"FBIS3-2172\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut maintenant construire l'histogramme des interactions entre les embeddings de la requête et ceux du document.\n",
    "\n",
    "On prend comme exemple 4 bins: ${[-1, -0.5]}$ ${[-0.5, 0]}$ ${[0, 0.5]}$ ${[0.5, 1]}$.\n",
    "\n",
    "**Plusieurs manières de construire un histogramme**: compter le nombre de valeurs, compter puis normaliser..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import random\n",
    "\n",
    "class DRMMinputGenerator:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, query_term_maxlen, intervals, engine, indexname, normalize=False):\n",
    "        self.q_max_len = query_term_maxlen\n",
    "        self.intervals = intervals\n",
    "        self.normalize = normalize\n",
    "        self.intvlsArray = np.linspace(-1, 1, self.intervals)\n",
    "        self.engine = engine\n",
    "        self.indexname = indexname #pour interroger la bonne collection\n",
    "        \n",
    "    def set_params(self, model_wv, vectoriser, idf_file=\"idf_robust2004.pkl\"):\n",
    "        self.model_wv = model_wv\n",
    "        self.vectoriser = vectoriser\n",
    "        #dict: term -> idf\n",
    "        self.idf_values = pickle.load(open(idf_file, \"rb\"))\n",
    "    \n",
    "    def get_idf_vec(self, q):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        vec = np.zeros(self.q_max_len)\n",
    "        for i, term in enumerate(q.split()):\n",
    "            if term in self.model_wv: #je suis pas sûr de mon coup là\n",
    "                if term in self.idf_values:\n",
    "                    vec[i] = self.idf_values[term]\n",
    "                elif term.lower() in self.idf_values:\n",
    "                    vec[i] = self.idf_values[term.lower()]\n",
    "                else:\n",
    "                    vec[i] = 1.\n",
    "                \n",
    "        return vec\n",
    "        \n",
    "    def hist(self, query, document):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for i in query.nonzero()[1]:\n",
    "            histo = []\n",
    "            for j in document.nonzero()[1]:\n",
    "                histo.append(cosine_similarity([self.model_wv.vectors[i]], [self.model_wv.vectors[j]])[0][0])\n",
    "            histo, bin_edges = np.histogram(histo, bins=self.intvlsArray)\n",
    "            if self.normalize:\n",
    "                histo = histo / histo.sum()\n",
    "            X.append(histo)\n",
    "        if len(query.nonzero()[1]) < self.q_max_len:\n",
    "            # compléter avec des zéro\n",
    "            for i in range(self.q_max_len - len(query.nonzero()[1])):\n",
    "                X.append([0]*self.intervals)\n",
    "        #retourner histogramme interactions\n",
    "        return np.array(X)\n",
    "    \n",
    "    def load_data(self, queries, paires, test_size=0.2):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        #spliter les requêtes en train/test\n",
    "        lol = [q for q in queries.keys() if q in paires]\n",
    "        random.shuffle(lol)\n",
    "        test_keys = lol[:int(test_size * len(lol))]\n",
    "        train_keys = lol[int(test_size * len(lol)):]\n",
    "        \n",
    "        #pour chaque requête on va générer autant de paires relevant que irrelevant\n",
    "        #pour nos besoins on va alterner paires positives et paires négatives\n",
    "        train_hist = [] # les histogrammes d'interraction\n",
    "        test_hist = []\n",
    "        train_idf = [] #les vecteurs d'idf\n",
    "        test_idf = []\n",
    "        \n",
    "        for requete in train_keys:\n",
    "            #recuperer les mots dont on connait les embeddings dans la query\n",
    "            q = self.vectoriser.transform([queries[requete]])\n",
    "            idf_vec = self.get_idf_vec(queries[requete])\n",
    "            for pos, neg in zip(paires[requete][\"relevant\"], paires[requete][\"irrelevant\"]):\n",
    "                #lire le doc, la requete et creer l'histogramme d'interraction\n",
    "                \n",
    "                #yanis fous la partie lecture de doc ici stp\n",
    "                doc = self.engine.get(index=self.indexname, id=pos)\n",
    "                #récupérer le texte\n",
    "                if doc['found'] == False:\n",
    "                    print(\"oulala erreur\")\n",
    "                d = self.vectoriser.transform([doc['_source']['text']])\n",
    "                train_hist.append(self.hist(q, d)) #append le doc positif\n",
    "                train_idf.append(idf_vec) #append le vecteur idf de la requête\n",
    "                \n",
    "                #yanis fous la partie lecture de doc ici stp\n",
    "                doc = self.engine.get(index=self.indexname, id=neg)\n",
    "                #récupérer le texte\n",
    "                if doc['found'] == False:\n",
    "                    print(\"oulala erreur\")\n",
    "                d = self.vectoriser.transform([doc['_source']['text']])\n",
    "                train_hist.append(self.hist(q, d)) #append le doc négatif\n",
    "                train_idf.append(idf_vec) #append le vecteur idf de la requête\n",
    "        train_labels = np.zeros(len(train_hist))\n",
    "        train_labels[::2] = 1\n",
    "        \n",
    "        \n",
    "        for requete in test_keys:\n",
    "            #recuperer les mots dont on connait les embeddings dans la query\n",
    "            q = self.vectoriser.transform([queries[requete]])\n",
    "            idf_vec = self.get_idf_vec(queries[requete])\n",
    "            for pos, neg in zip(paires[requete][\"relevant\"], paires[requete][\"irrelevant\"]):\n",
    "                #lire le doc, la requete et creer l'histogramme d'interraction\n",
    "                \n",
    "                #yanis fous la partie lecture de doc ici stp\n",
    "                doc = self.engine.get(index=self.indexname, id=pos)\n",
    "                #récupérer le texte\n",
    "                if doc['found'] == False:\n",
    "                    print(\"oulala erreur\")\n",
    "                d = self.vectoriser.transform([doc['_source']['text']])\n",
    "                test_hist.append(self.hist(q, d)) #append le doc positif\n",
    "                test_idf.append(idf_vec) #append le vecteur idf de la requête\n",
    "                \n",
    "                doc = self.engine.get(index=self.indexname, id=neg)\n",
    "                #récupérer le texte\n",
    "                if doc['found'] == False:\n",
    "                    print(\"oulala erreur\")\n",
    "                d = self.vectoriser.transform([doc['_source']['text']])\n",
    "                test_hist.append(self.hist(q, d)) #append le doc négatif\n",
    "                test_idf.append(idf_vec) #append le vecteur idf de la requête\n",
    "        test_labels = np.zeros(len(train_hist))\n",
    "        test_labels[::2] = 1\n",
    "        \n",
    "        return (train_hist, train_idf, train_labels), (test_hist, test_idf, test_labels)\n",
    "        \n",
    "        #éventuellement sauvegarder tout ça sur le disque comme ça c fait une bonne fois pour toutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputgenerator = DRMMinputGenerator(query_term_maxlen(queries), 5, engine, 'robust-2004')\n",
    "inputgenerator.set_params(model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.91303235, 4.32872259, 5.81042365, 0.        , 0.        ])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputgenerator.get_idf_vec(\"France Paris terrorist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'672'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-dc86ab13e901>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaires\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-104-3faae4dc0f3d>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(self, queries, paires, test_size)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectoriser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrequete\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0midf_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_idf_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrequete\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaires\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrequete\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"relevant\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaires\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrequete\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"irrelevant\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m                 \u001b[1;31m#lire le doc, la requete et creer l'histogramme d'interraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '672'"
     ]
    }
   ],
   "source": [
    "train, test = inputgenerator.load_data(queries, paires, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture du modèle\n",
    "\n",
    "D'après <a href=\"https://dl.acm.org/citation.cfm?id=2983769\">l'article</a>\n",
    "\n",
    "Code d'après <a href=\"https://github.com/sebastian-hofstaetter/neural-ranking-drmm/blob/master/neural-ranking/keras_model.py\">ce génie</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Activation, Lambda, Permute, merge\n",
    "from keras.layers import Reshape, Dot\n",
    "from keras.activations import softmax\n",
    "\n",
    "\n",
    "def build_keras_model(params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer_interactions = keras.initializers.RandomUniform(minval=-0.1, maxval=0.1, seed=11)\n",
    "    initializer_gating = keras.initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=11)\n",
    "    \n",
    "    #input interactions\n",
    "    interactions = Input(name='interactions', shape=(params['query_term_maxlen'], params['hist_size']))\n",
    "    \n",
    "    #input des term vectors de la query\n",
    "    query = Input(name='term_vector', shape=(params['query_term_maxlen'],1))\n",
    "\n",
    "    #partie feed forward\n",
    "    z = interactions\n",
    "    for i in range(len(params['hidden_sizes'])):\n",
    "        z = Dense(params['hidden_sizes'][i], kernel_initializer=initializer_interactions, name=\"dense_layer_{}\".format(i))(z)\n",
    "        z = Activation('tanh', name=\"activation_of_layer_{}\".format(i))(z)\n",
    "\n",
    "    z = Permute((2, 1))(z)\n",
    "    z = Reshape((params['query_term_maxlen'],))(z)\n",
    "\n",
    "    #la partie term gating\n",
    "    q_w = Dense(1, kernel_initializer=initializer_gating, use_bias=False, name=\"gating_W\")(query)\n",
    "    q_w = Lambda(lambda x: softmax(x, axis=1), output_shape=(params['query_term_maxlen'],))(q_w)\n",
    "    q_w = Reshape((params[\"query_term_maxlen\"],))(q_w)\n",
    "\n",
    "    # combination of softmax(query term idf) and feed forward result per query term\n",
    "    out_ = Dot(axes=[1, 1], name='s')([z, q_w])\n",
    "\n",
    "    model = Model(inputs=[query, interactions], outputs=[out_])\n",
    "\n",
    "    return model\n",
    "\n",
    "# from https://github.com/faneshion/MatchZoo/blob/master/matchzoo/losses/rank_losses.py\n",
    "from keras.backend import tf\n",
    "from keras.losses import *\n",
    "from keras.layers import Lambda\n",
    "# y_true is IGNORED (!), you don't have to set a label to train (?)\n",
    "# y_pred contains the complete batch (!)\n",
    "#  -> the slicing splits the tensors in even and odd (pos and negative from the input)\n",
    "#  -> VERY IMPORTANT: The input data must not be shuffled !! shuffle = False\n",
    "\n",
    "def rank_hinge_loss(y_true, y_pred):\n",
    "\n",
    "    y_pos = Lambda(lambda a: a[::2, :], output_shape= (1,))(y_pred)\n",
    "    y_neg = Lambda(lambda a: a[1::2, :], output_shape= (1,))(y_pred)\n",
    "    \n",
    "    loss = K.maximum(0., 1. + y_neg - y_pos)\n",
    "    return K.mean(loss)\n",
    "\n",
    "#du coup pour l'entrainement dans le batch il faut aligner des paires de doc de la forme \n",
    "#positif-négatif-positif-négatif... etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "interactions (InputLayer)       (None, 4, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_0 (Dense)           (None, 4, 5)         30          interactions[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_of_layer_0 (Activati (None, 4, 5)         0           dense_layer_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 4, 32)        192         activation_of_layer_0[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_of_layer_1 (Activati (None, 4, 32)        0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 4, 1)         33          activation_of_layer_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "term_vector (InputLayer)        (None, 4, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_of_layer_2 (Activati (None, 4, 1)         0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating_W (Dense)                (None, 4, 1)         1           term_vector[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute_22 (Permute)            (None, 1, 4)         0           activation_of_layer_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 4)            0           gating_W[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_43 (Reshape)            (None, 4)            0           permute_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_44 (Reshape)            (None, 4)            0           lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "s (Dot)                         (None, 1)            0           reshape_43[0][0]                 \n",
      "                                                                 reshape_44[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 256\n",
      "Trainable params: 256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"hidden_sizes\": [5, 32, 1],\n",
    "    \"hist_size\": 5,\n",
    "    \"query_term_maxlen\": 4,\n",
    "    \"embedding_size\": 300\n",
    "}\n",
    "\n",
    "drmm = build_keras_model(params)\n",
    "drmm.compile(loss=rank_hinge_loss, optimizer='adam')\n",
    "drmm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-2d259890b1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mann_viz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Artificial Neural network - Model Visualization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ann_visualizer/visualize.py\u001b[0m in \u001b[0;36mann_viz\u001b[0;34m(model, view, filename, title)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mhidden_layers_nr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "ann_viz(drmm, title=\"Artificial Neural network - Model Visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
