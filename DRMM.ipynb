{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRMM (Deep Relevance Matching Model)\n",
    "\n",
    "_Ismaël Bonneau_\n",
    "\n",
    "Mots-clés: _Relevance Matching_, _Semantic Matching_, _Neural Models_,\n",
    "_Ad-hoc Retrieval_, _Ranking Models_\n",
    "\n",
    "\n",
    "#### But de ce notebook: Comprendre et construire une architecture **DRMM** fonctionnelle avec **pytorch**, et l'expliquer de façon concise.\n",
    "\n",
    "Un gros bisou à Daniel Godoy pour son tutoriel <a href=\"https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\">comprendre pytorch par l'exemple pas à pas</a> (en Anglais).\n",
    "\n",
    "Pour cela, 2 étapes:\n",
    "\n",
    "- construire la chaîne de pré traitements:\n",
    "    - générer des paires document-requête non pertinentes et pertinentes pour l'apprentissage\n",
    "    - générer des histogrammes d'interaction locales au niveau document-requête\n",
    "- construire l'architecture DRMM\n",
    "\n",
    "Les interractions sont pour le moment des interactions locales sur des word embeddings et sont mesurées comme une similarité cosinus entre les vecteurs des mots de la requête et ceux du document.\n",
    "\n",
    "## En quoi consiste le modèle DRMM?\n",
    "\n",
    "lien vers <a href=\"https://arxiv.org/pdf/1711.08611.pdf\">l'article</a> (en Anglais) par _Jiafeng Guo_, _Yixing Fan_, _Qingyao Ai_, _W. Bruce Croft_ (2017) [1]\n",
    "\n",
    "DRMM (Deep Relevance Matching Model) est un modèle de réseau de neurones profond pour la RI (recherche d'information).\n",
    "\n",
    "l'architecture DRMM est une architecture de modèle **orienté interactions**, qui applique une fonction apprise (un réseau de neurones n'est rien d'autres qu'une fonction très complexe) à des interactions entre un document et une requête, qui ne font donc pas partie du réseau et ne sont pas apprises. Cette fonction a pour but de calculer un **score pour la paire document-requête**, score d'autant **plus élevé que le document est pertinent pour la requête**.\n",
    "\n",
    "Il se compose de deux parties: \n",
    "\n",
    "- Une partie **\"feed forward matching network\"** qui est un simple perceptron multicouche. Il s'agit dans l'implémentation de [1] d'un perceptron à 3 couches, de taille 30 neurones, 5 neurones, et 1 neurone. Cette partie vient calculer un score pour l'interaction de chaque terme de la requête avec l'ensemble des termes du document. Pour une requête de 5 termes, le partie feed forward matching produira donc un vecteur de dimension 5, pour 1 score par terme de la requête. On a donc un bloc qui prend en entrée une matrice d'interactions des termes de la requête, et qui rend en sortie un vecteur de score pour chaque terme de la requête.\n",
    "\n",
    "- Une partie **\"term gating network\"** qui est un perceptron à une couche. Il s'agit uniquement d'apprendre un vecteur qui vient mutiplier chaque \"vecteur de terme\" de la requête et ensuitepondérer les scores calculés par la partie feed forward. \n",
    "\n",
    "<img src=\"images/DRMMschema.png\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import sep\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "embeddings_path = \"embeddings_wiki2017\"\n",
    "dataset_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré traitements: \n",
    "\n",
    "### Récupérer des word embeddings \n",
    "\n",
    "Ce word embedding a les caractéristiques suivantes:\n",
    "\n",
    "- FastText Continuous Skipgram\n",
    "- taille de vecteur ${300}$\n",
    "- window ${5}$\n",
    "- entrainé sur wikipédia février 2017 en langue anglaise\n",
    "- pas de lemmatisation\n",
    "- ${302815}$ mots\n",
    "\n",
    "http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText #classe deprecated\n",
    "\n",
    "model = FastText.load_fasttext_format('embeddings_wiki2017/parameters.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "On utilise pour cet exemple le dataset <a href=\"https://trec.nist.gov/data/robust/04.guidelines.html\">robust 2004</a>. Il s'agit d'une collection de documents datés de 2004 provenant de journaux américains, le LA times, le Financial times, le Federal Register 94, et le Foreign Broadcast Information Service.\n",
    "\n",
    "\n",
    "| nombre de requêtes      |\n",
    "| ------------- |\n",
    "|     **250**       |\n",
    "\n",
    "| collection      |     nombre de documents    |     taille en Mo    |\n",
    "| ------------- |: -------------: |: -------------: |\n",
    "| Financial times      |        210 158        |        564        |\n",
    "| La times       |        131,896        |        475        |\n",
    "| FBIS      |        130,471        |        470        |\n",
    "| FR94      |        55,630        |        395        |\n",
    "| **TOTAL**      |        **528,155**        |        **1904**        |\n",
    "\n",
    "\n",
    "### On Récupère les paires de pertinence/non pertinence pour chaque requête \n",
    "\n",
    "On génère un dictionnaire qui contient pour chaque requête en clé, un dictionnaire contenant 2 listes:\n",
    "- \"relevant\" contenant des id de document pertinents pour la requête.\n",
    "- \"irrelevant\" contenant des id de document non pertinents pour la requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paires = {}\n",
    "\n",
    "with open(dataset_path + sep + \"qrels.robust2004.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        lol = line.split()\n",
    "        paires.setdefault(lol[0], {})\n",
    "        paires[lol[0]].setdefault('relevant', []) \n",
    "        paires[lol[0]].setdefault('irrelevant', [])\n",
    "        if lol[-1] == '1':\n",
    "            paires[lol[0]][\"relevant\"].append(lol[2])\n",
    "        else:\n",
    "            paires[lol[0]][\"irrelevant\"].append(lol[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On récupère les requêtes:\n",
    "\n",
    "Elles se trouvent sous forme de tuple ([mots clés], [texte de la requête]). On ne garde que les mots clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation\n",
    "\n",
    "def clean(txt):\n",
    "    return txt.replace(\",\", \"\").replace(\".\", \"\")\n",
    "\n",
    "with open(dataset_path + sep + \"robust2004.txt\", \"r\") as f:\n",
    "    queries = ast.literal_eval(f.read())\n",
    "queries = {d:clean(queries[d][0]) for d in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international organized crime\n",
      "foreign minorities germany\n"
     ]
    }
   ],
   "source": [
    "print(queries[\"301\"])\n",
    "print(queries[\"401\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krovetz #stemmer pas mal\n",
    "\n",
    "# Krovetz stemmer est un stemmer moins \"destructif\" que le porter.\n",
    "# Viewing morphology as an inference process: https://dl.acm.org/citation.cfm?id=160718\n",
    "\n",
    "ks = krovetz.PyKrovetzStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le DRMM a deux entrées: une entrée interactions et une entrée termes.\n",
    "\n",
    "L'entrée termes prend un vecteur d'idf des termes de la requête. Il faut donc pouvoir récupérer efficacement des idf. Pour cela, on construit un dictionnaire terme -> idf qui nous servira dans l'étape d'après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bon là on charge du coup vu que le fichier est sauvegardé sur le disque\n",
    "idf = pickle.load(open(\"idf_robust2004.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On prépare dans des fichiers les matrices d'interactions et les vecteurs d'idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query chargé\n",
      "relevance chargé\n",
      "docs chargés\n"
     ]
    }
   ],
   "source": [
    "from datasets import Robust04 #permet de gérer le chargement et le traitement de robust2004\n",
    "\n",
    "inputgenerator = Robust04(intervals=30, model_wv=model)\n",
    "inputgenerator.load_idf(idf_file=\"idf_robust2004.pkl\")\n",
    "inputgenerator.load_all_query(file_query=\"data/robust2004.txt\")\n",
    "inputgenerator.load_relevance(file_rel=\"data/qrels.robust2004.txt\")\n",
    "inputgenerator.load_all_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette méthode calcule les matrices d'interraction et les charge dans des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de requetes: 249.\n",
      "data completed\n"
     ]
    }
   ],
   "source": [
    "inputgenerator.prepare_data_forNN(\"saved_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette méthode pré calcule les matrices d'interraction des résultats de BM25 et les charge dans des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputgenerator.prepare_data_reranking(pickle.load(open(\"results_bm25_robust.pkl\", \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On peut maintenant récupérer les matrices d'interraction pour les utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 requetes en train, 48 en test.\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2 # 20% de requêtes en test\n",
    "\n",
    "#random.seed(1401)\n",
    "\n",
    "query_idf = pickle.load(open(\"saved_data/query_idf.pkl\", \"rb\"))\n",
    "\n",
    "interractions_train_pos = []\n",
    "interractions_train_neg = []\n",
    "interractions_test_pos = []\n",
    "interractions_test_neg = []\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for id_requete in paires:\n",
    "    if id_requete != '634':\n",
    "        saintjeanlapuenta = np.load(\"saved_data/{}_interractions.npy\".format(id_requete))\n",
    "        if random.random() < test_size:\n",
    "            test.append(id_requete)\n",
    "            for neg, pos in zip(saintjeanlapuenta[::2], saintjeanlapuenta[1::2]):\n",
    "                interractions_test_pos.append([torch.from_numpy(pos).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "                interractions_test_neg.append([torch.from_numpy(neg).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "        else:\n",
    "            train.append(id_requete)\n",
    "            for neg, pos in zip(saintjeanlapuenta[::2], saintjeanlapuenta[1::2]):\n",
    "                interractions_train_pos.append([torch.from_numpy(pos).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "                interractions_train_neg.append([torch.from_numpy(neg).float(), torch.from_numpy(np.array([query_idf[id_requete]])).float()])\n",
    "\n",
    "print(\"{} requetes en train, {} en test.\".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture du modèle\n",
    "\n",
    "### Avec pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MarginRankingLoss\n",
    "\n",
    "#hérite de la classe Pytorch Module\n",
    "class DRMM(torch.nn.Module):\n",
    "    def __init__(self, hist_size, query_term_maxlen, hidden_sizes=[128,64,16,1], use_cuda=True):\n",
    "        \n",
    "        if hidden_sizes[-1] != 1:\n",
    "            raise ValueError(\"la taille du dernier layer doit être 1\")\n",
    "            \n",
    "        super(DRMM, self).__init__()\n",
    "        self.MLP = [torch.nn.Linear(hist_size, hidden_sizes[0])] + [\n",
    "            torch.nn.Linear(hidden_sizes[i-1], hidden_sizes[i]) for i in range(1, len(hidden_sizes))]\n",
    "        for i in range(len(self.MLP)):\n",
    "            torch.nn.init.xavier_normal_(self.MLP[i].weight, gain = torch.nn.init.calculate_gain('tanh'))\n",
    "            \n",
    "        #initialisation du vecteur de term gating\n",
    "        self.termgating = torch.nn.Linear(query_term_maxlen, query_term_maxlen, bias=False)\n",
    "        torch.nn.init.xavier_normal_(self.termgating.weight, gain = torch.nn.init.calculate_gain('tanh'))\n",
    "        \n",
    "    #méthode forward à redéfinir\n",
    "    def forward(self, interractions, termvector):\n",
    "        \"\"\"\n",
    "        interractions: (query_term_maxlen, hist_size)\n",
    "        termvector: \n",
    "        \"\"\"\n",
    "        #partie histogramme\n",
    "        interractions_output = self.MLP[0](interractions)\n",
    "        interractions_output = torch.nn.functional.tanh(interractions_output)\n",
    "        for layer in self.MLP[1:]:\n",
    "            interractions_output = layer(interractions_output)\n",
    "            interractions_output = torch.nn.functional.tanh(interractions_output)\n",
    "        interractions_output = interractions_output.squeeze() #passe de (query_term_maxlen, 1) à (1, query_term_maxlen)\n",
    "        #partie term gating \n",
    "        gating_output = torch.nn.functional.softmax(self.termgating(termvector)).squeeze()\n",
    "        #combiner les 2 avec un produit scalaire\n",
    "        axis = 0\n",
    "        #si on travaille par mini-batch:\n",
    "        if len(interractions.size()) == 3:\n",
    "            axis = 1\n",
    "        s = torch.sum(interractions_output * gating_output, dim = axis) #dim = 0 ne marche qui pas batch.\n",
    "        return s\n",
    "    \n",
    "    def get_model_size(self):\n",
    "        \"\"\"\n",
    "        retourne le nombre de paramètres du modèle.\n",
    "        \"\"\"\n",
    "        return sum([p.size(0) if len(p.size()) == 1 else p.size(0)*p.size(1) for p in self.parameters()] +\n",
    "                  [p.weight.size(0)*p.weight.size(1) for p in self.MLP])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"DRMM with {} parameters. ti as compris\".format(self.get_model_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On va créer une classe de Dataset qui contient les paires pertinentes et les paires non pertinentes.\n",
    "\n",
    "Plus précisément, la classe Dataset de Pytorch est un wrapper pour X (traditionnellement les vecteurs de features) et Y (traditionnellement les labels). Ici, on a pas besoin de vecteur de labels. On peut donc transformer l'utilisation de la classe pour mettre dans X une matrice de paires (interractions relevant, vecteur de termes relevant) et dans Y la même matrice, pour les paires non pertinentes. L'avantage est que lors du shuffle avec le DataLoader, on conservera l'alignement doc pertinent/non pertinent pour une requête et on ne sera pas embêté pour utiliser la ranking hinge loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DrmmDataset(Dataset):\n",
    "    def __init__(self, pos_tensor, neg_tensor):\n",
    "        self.x = pos_tensor\n",
    "        self.y = neg_tensor\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DrmmDataset(interractions_train_pos, \n",
    "                            interractions_train_neg)\n",
    "val_dataset = DrmmDataset(interractions_test_pos,\n",
    "                          interractions_test_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'étape de création du DataLoader:\n",
    "\n",
    "Cette classe va nous permettre de gérer le découpage en mini batches et le shuffle. C'est un wrapper pour la classe DataSet qui fournit entre autres un itérateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batchsize = 20\n",
    "#classe utile pour gérer les mini batches et le shuffle (crucial!)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batchsize, shuffle=True)\n",
    "#shuffle = False pour le DataLoader de test!\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On regroupe toutes les étapes de forward/backprop sur un mini-batch dans une fonction:\n",
    "\n",
    "La fonction prend deux mini batch en entrée et:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drmm_make_train_step(model, loss_fn, optimizer):\n",
    "    def drmm_train_step(pos_batch, neg_batch):\n",
    "        #mettre le modèle en mode train\n",
    "        model.train()\n",
    "        #étape forward...\n",
    "        pos_score = model(pos_batch[0], pos_batch[1])\n",
    "        neg_score = model(neg_batch[0], neg_batch[1])\n",
    "        #calcul de la loss\n",
    "        loss = loss_fn(pos_score, neg_score, torch.Tensor([1] * pos_batch[0].size()[0]))\n",
    "        #calcul des gradients\n",
    "        loss.backward()\n",
    "        #mise à jour des paramètres\n",
    "        optimizer.step()\n",
    "        #reset des gradients après le passage sur ce batch\n",
    "        optimizer.zero_grad()\n",
    "        #retourner la loss\n",
    "        return loss.item() #.item()\n",
    "    \n",
    "    return drmm_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drmm_make_val_step(model, loss_fn, optimizer):\n",
    "    def drmm_val_step(pos_batch, neg_batch):\n",
    "        #mettre le modèle en mode test\n",
    "        model.eval()\n",
    "        #étape forward...\n",
    "        pos_score = model(pos_batch[0], pos_batch[1])\n",
    "        neg_score = model(neg_batch[0], neg_batch[1])\n",
    "        \n",
    "        loss = loss_fn(pos_score, neg_score, torch.Tensor([1] * pos_batch[0].size()[0]))\n",
    "        \n",
    "        return loss.item()\n",
    "    return drmm_val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRMM with 180 parameters. ti as compris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-2b6d8130498b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#neg_batch[1]: les query term correspondantes...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtiacompris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiacompris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-f02f92b4500b>\u001b[0m in \u001b[0;36mdrmm_train_step\u001b[0;34m(pos_batch, neg_batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpos_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#calcul des gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#mise à jour des paramètres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "drmm = DRMM(30, 5, hidden_sizes=[5,1])\n",
    "print(drmm)\n",
    "\n",
    "hingeloss = MarginRankingLoss()\n",
    "optimizer = torch.optim.Adagrad(drmm.parameters(), lr=0.001)\n",
    "\n",
    "make_train_step = drmm_make_train_step(drmm, hingeloss, optimizer)\n",
    "make_val_step = drmm_make_val_step(drmm, hingeloss, optimizer)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "nb_epochs = 300\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    #parcourir tous les batches du dataloader\n",
    "    tiacompris = []\n",
    "    for pos_batch, neg_batch in train_loader:\n",
    "\n",
    "        #pos_batch[0]: les interractions positives\n",
    "        #pos_batch[1]: les query term\n",
    "        #neg_batch[0]: les interractions négatives correspondantes...\n",
    "        #neg_batch[1]: les query term correspondantes...\n",
    "\n",
    "        tiacompris.append(make_train_step(pos_batch, neg_batch))\n",
    "    losses.append(np.array(tiacompris).mean())\n",
    "    \n",
    "    #on met le modèle en mode \"ne calcule pas le gradient\"\n",
    "    with torch.no_grad():\n",
    "        #on parcourt\n",
    "        for pos_batch_val, neg_batch_val in val_loader:    \n",
    "            val_loss = make_val_step(pos_batch_val, neg_batch_val)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(list(range(nb_epochs)), losses, label=\"train\")\n",
    "plt.plot(list(range(nb_epochs)), val_losses, label=\"validation\")\n",
    "plt.title(\"DRRM loss sur robust-2004\", fontsize=16)\n",
    "plt.ylabel(\"loss\", fontsize=13)\n",
    "plt.xlabel(\"epochs\", fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut maintenant tester le modèle avec des métriques de RI:\n",
    "\n",
    "Pour chacune des requêtes de test, on récupère les 2000 premiers documents retournés par *BM25*. Nous procédons en réordonnant cette liste de documents avec notre modèle *DRMM* appris, et nous calculons des performances sur les 1000 premiers documents de ce re ranking sur un ensemble de métriques de RI (**NDCG@20**, **MAP**, **P@20**), en le comparant avec le ranking original *BM25*.\n",
    "\n",
    "Pour BM25, l'indexation a été faite comme suit:\n",
    "\n",
    "- indexation sur le texte du document et non le titre\n",
    "- texte sans stopwords, ponctuation, balises\n",
    "- pas de lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reranker:\n",
    "    def __init__(self, bm25_dict):\n",
    "        self.bm25_dict = bm25_dict #le dictionnaire query -> 2000 docs relevants pour bm25\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def rerank(self, queries=None):\n",
    "        \n",
    "        #queries: une liste d'ID de requete pour lesquels on veut réordonner les résultats\n",
    "        if queries == None:\n",
    "            queries_to_rerank = list(self.bm25_dict.keys())\n",
    "        else:\n",
    "            queries_to_rerank = queries\n",
    "        \n",
    "        #pour chaque requete, on va charger réordonner ses résultats avec le modèle\n",
    "        query_idf = pickle.load(open(\"saved_data/query_idf.pkl\", \"rb\"))\n",
    "        reranked_dict = {}\n",
    "        for id_requete in queries_to_rerank:\n",
    "            if id_requete != \"634\":\n",
    "                with torch.no_grad():\n",
    "                    #contient une matrice (2000, query_max_len, hist_size)\n",
    "                    saintjeanlapuenta = np.load(\"data/bm25_robust/{}_interractions.npy\".format(id_requete))\n",
    "                    a = torch.from_numpy(np.tile(np.array([query_idf[id_requete]]), (saintjeanlapuenta.shape[0],1))).float()\n",
    "\n",
    "                    model_scores = self.model(torch.from_numpy(saintjeanlapuenta).float(), a).data.numpy()\n",
    "                    lol = np.argsort(model_scores)[::-1] #tri décroissant\n",
    "\n",
    "                    # reranked: liste de tuples (document_id, score)\n",
    "                    reranked_dict[id_requete] = [(self.bm25_dict[id_requete][i][0], model_scores[i]) for i in lol]\n",
    "\n",
    "        return reranked_dict\n",
    "    \n",
    "    def get_results(self, id_requete, rank_list):\n",
    "        results = []\n",
    "        for i, (doc_id, score) in enumerate(rank_list[:1000]):\n",
    "            results.append(\" \".join([id_requete, \"Q0\", doc_id, str(i + 1), str(score), \"EARIA\"]))\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def save_results(self, rank_dict, res_file):\n",
    "        \"\"\"\n",
    "        sauver sur un fichier au format attendu par TREC\n",
    "        un dictionnaire query_id -> list (doc_id, score)\n",
    "        \"\"\"\n",
    "        results = [f\"{id_requete} Q0 EMPTY 1001 -100000 EARIA\" for id_requete in rank_dict]\n",
    "        for id_requete in rank_dict:\n",
    "            results.extend(self.get_results(id_requete, rank_dict[id_requete]))\n",
    "        \n",
    "        with open(res_file, \"w\") as tiacompris:\n",
    "            tiacompris.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def compute_trec_eval(qrel_file_path, resp_file_name):\n",
    "    command = [\"trec_eval/trec_eval\" , \"-c\", \"-M1001\", \"-m\", \"map\",\"-m\", \"P.20\", qrel_file_path, resp_file_name]\n",
    "    #Le fichier trec_eval est à récuperer ici https://github.com/usnistgov/trec_eval\n",
    "    # une fois clone, il faut faire un make dans le dossier et autoriser à l'execution (chmod +x)\n",
    "    completed_process = subprocess.run(command, capture_output=True)\n",
    "    results = completed_process.stdout.decode(\"utf-8\")\n",
    "    \n",
    "    with open(\"caca\", \"w\") as f:\n",
    "        f.write(results)\n",
    "    \n",
    "    total_score = {}\n",
    "    for tkt in results.split(\"\\n\")[:2]:\n",
    "        total_score[tkt.split(\"\\t\")[0].strip()] = float(tkt.split(\"\\t\")[2].strip())\n",
    "    \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(id_requete, rank_list):\n",
    "    results = []\n",
    "    for i, (doc_id, score) in enumerate(rank_list[:1000]):\n",
    "        results.append(\" \".join([id_requete, \"Q0\", doc_id, str(i + 1), str(score), \"EARIA\"]))\n",
    "    return results\n",
    "            \n",
    "    \n",
    "def save_results(rank_dict, res_file, queries):\n",
    "    \"\"\"\n",
    "    sauver sur un fichier au format attendu par TREC\n",
    "    un dictionnaire query_id -> list (doc_id, score)\n",
    "    \"\"\"\n",
    "    results = [f\"{id_requete} Q0 EMPTY 1001 -100000 EARIA\" for id_requete in queries]\n",
    "    for id_requete in queries:\n",
    "        results.extend(get_results(id_requete, rank_dict[id_requete]))\n",
    "        \n",
    "    with open(res_file, \"w\") as tiacompris:\n",
    "        tiacompris.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP et précision avec bm25 seul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': 0.0541, 'P_20': 0.0729}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_dict = pickle.load(open(\"results_bm25_robust.pkl\", \"rb\"))\n",
    "save_results(bm25_dict, \"proutpipi\", queries=test)\n",
    "compute_trec_eval(\"data/qrels.robust2004.txt\", \"proutpipi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP et précision sur le re ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map': 0.0346, 'P_20': 0.0663}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_dict = pickle.load(open(\"results_bm25_robust.pkl\", \"rb\"))\n",
    "reranker = Reranker(bm25_dict)\n",
    "reranker.set_model(drmm)\n",
    "lol = reranker.rerank(queries = train)\n",
    "reranker.save_results(lol, \"tiacompris\")\n",
    "compute_trec_eval(\"data/qrels.robust2004.txt\", \"tiacompris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perf en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map': 0.0132, 'P_20': 0.0165}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = reranker.rerank(queries = test)\n",
    "reranker.save_results(lol, \"stfe\")\n",
    "compute_trec_eval(\"data/qrels.robust2004.txt\", \"stfe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographie\n",
    "\n",
    "\n",
    "[1] <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.161&rep=rep1&type=pdf\">What Works Better for Question Answering: Stemming or\n",
    "Morphological Query Expansion?</a>\n",
    "\n",
    "[2] <a href=\"https://dl.acm.org/citation.cfm?id=160718\" >Viewing morphology as an inference process</a>\n",
    "\n",
    "[3] <a href=\"https://arxiv.org/pdf/1809.01682.pdf\">Deep Relevance Ranking Using Enhanced Document-Query Interactions</a>\n",
    "\n",
    "[4] <a href=\"http://delivery.acm.org/10.1145/2810000/2806475/p1411-kenter.pdf?ip=132.227.125.83&id=2806475&acc=ACTIVE%20SERVICE&key=7EBF6E77E86B478F%2EA72B4D473219EA0C%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1562588994_ca050bde6ad37b2c0d433db2e5df63ba\">Short Text Similarity with Word Embeddings</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
